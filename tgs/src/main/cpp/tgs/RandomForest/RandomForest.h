/*
 * This file is part of Hootenanny.
 *
 * Hootenanny is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 * 
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 * --------------------------------------------------------------------
 *
 * The following copyright notices are generated automatically. If you
 * have a new notice to add, please use the format:
 * " * @copyright Copyright ..."
 * This will properly maintain the copyright information. DigitalGlobe
 * copyrights will be updated automatically.
 *
 * @copyright Copyright (C) 2015 DigitalGlobe (http://www.digitalglobe.com/)
 */

#ifndef __RANDOM_FOREST_H__
#define __RANDOM_FOREST_H__

//STL Includes
#include <map>
#include <set>
#include <string>
#include <utility>
#include <vector>

//Urgent Includes
#include "../TgsExport.h"
#include "DataFrame.h"
#include "RandomTree.h"

namespace Tgs
{
  /**
  *  The RandomForest is an implementation of the Leo Breiman
  * Random Forest Classification algorithm
  *
  * For more information see:
  * http://oz.berkeley.edu/~breiman/RandomForests/
  */
  class TGS_EXPORT RandomForest
  {
  public:
    /**
    * Constructor
    */
    RandomForest();

    /**
    * Destructor
    */
    ~RandomForest();

    /**
    * Classifies a data vector against a generated random forest.  
    *
    * The vector is classified against each tree in the forest and the final classification is the
    * result of majority vote for each tree.
    *
    * @param dataVector a single data vector of size N where N is the number of factors
    * @param scores a map to hold the classification results as class name mapped to probability
    */
    void classifyVector(std::vector<double> & dataVector, std::map<std::string, double> & scores)
      const;

    /**
    *  Clears the random forest of its trees.
    */
    void clear();

    /**
    *  Exports the random forest to a file
    *
    * @param fileStream the stream to write the information
    */
    void exportModel(std::ostream & fileStream, std::string tabDepth = "");

    /**
    * Finds the average classification error statistic for the forest based on the 
    * oob sets for the trees
    *
    * @param data the data set to operate upon
    * @param average variable to hold the computed average error
    * @param stdDev variable to hold the computed standard deviation
    */
    void findAverageError(DataFrame & data, double & average, double & stdDev);
    
    /**
    * Computes the proximity of the data vectors in the data set by running the 
    * complete data set through the tree and then tracking which vectors
    * were classified to the same node
    * 
    * @param data the set of data vectors
    * @param proximity a n x n (where n is the number of total data vectors) adjacency matrix
    */
    void findProximity(DataFrame & data, std::vector<unsigned int> & proximity);

    /**
    * This generates a text file containing the raw probability scores and a text file
    * containing the confusion matrix along with generated balanced *accuracy
    */
    void generateResults(std::string fileName);

    /**
    *  Gets the factor importance as generated by the random forest the highest
    *  values correspond to most important factors.
    *
    *  @param data the original data set 
    *  @param factorImportance a map of factor labels to purity improvement
    */
    void getFactorImportance(DataFrame & data, 
      std::map<std::string, double> & factorImportance);

    /**
     * Return a vector of the factor labels used to train this random forest.
     */
    const vector<string>& getFactorLabels() const { return _factorLabels; }

    /**
    *  Imports the random forest from a file
    *
    * @param fileStream the stream to read the information
    */
    void import(std::istream & fileStream);

    /**
    *  @return true if the forest has been trained
    */
    bool isTrained(){return _forestCreated;}

    /**
    * Build the forest from a data set
    *
    * @param data the data set to train on 
    * @param numTrees the number of random trees to create
    * @param numFactors the number of factors to randomly choose as candidates for node splitting
    * @param posClass the name of the positive class
    * @param nodeSize the minimum number of data vectors in a set to split a node 
    * @param retrain fraction of top factors to use in retraining model (1.0 means use all factors and no retraining)
    * @param balanced true if the forest will be balanced
    */
    void trainBinary(DataFrame & data, unsigned int numTrees, unsigned int numFactors, 
      std::string posClass, unsigned int nodeSize = 1, double retrain = 1.0,
      bool balanced = false);

    /**
    * Build the forest from a data set
    *
    * @param data the data set to train on 
    * @param numTrees the number of random trees to create
    * @param numFactors the number of factors to randomly choose as candidates for node splitting
    * @param nodeSize the minimum number of data vectors in a set to split a node 
    * @param retrain fraction of top factors to use in retraining model (1.0 means use all factors and no retraining)
    * @param balanced true if the forest will be balanced
    */
    void trainMulticlass(DataFrame & data, unsigned int numTrees, unsigned int numFactors, 
      unsigned int nodeSize = 1, double retrain = 1.0, bool balanced = false);

    /**
    * Build the forest from a data set
    *
    * @param data the data set to train on 
    * @param numTrees the number of random trees to create
    * @param numFactors the number of factors to randomly choose as candidates for node splitting
    * @param posClass the name of the positive class
    * @param negClass the name of the negative class
    * @param nodeSize the minimum number of data vectors in a set to split a node 
    * @param retrain fraction of top factors to use in retraining model (1.0 means use all factors and no retraining)
    * @param balanced true if the forest will be balanced
    */
    void trainRoundRobin(DataFrame & data, unsigned int numTrees, unsigned int numFactors, 
      std::string posClass, std::string negClass, unsigned int nodeSize = 1, double retrain = 1.0,
      bool balanced = false);

  private:
   
    std::vector<RandomTree> _forest; /// A container for the random forest
    unsigned int _numSplitFactors;  /// The number of factors to test to split a node
    unsigned int _nodeSize;  /// The minimum number of data vectors in a set to split a node
    vector<string> _factorLabels; /// Labels for all the factors used in training.
    
    bool _forestCreated;  /// A flag to indicate if the forest was created successfully

    /// import factor labels from XML
    void _importFactorLabels(string &fileStream);
  };
}  //End namespace
#endif


