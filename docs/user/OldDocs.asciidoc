
== Using Hootenanny

Hootenanny can be accessed from both command line and via a simple web user interface (Hootenanny-iD) built on top of the link:$$https://www.openstreetmap.org/edit?editor=id$$[OpenStreetMap iD] Editor.  At its core, Hootenanny  provides a set of tools for manipulating and conflating vector data as well as translating data between various out of the box supported reference tag schemas such as OSM, TDS, UTP, MGCP.  As an extension to the existing translation capabilities within Hootenanny, the Hootenanny-iD web interface provides a simple translation tool known as the _Translation Assistant_ that will walk users through a custom translation of any customer data format into a supported tag schema.  Additional background on the Hootenanny-iD interface and Translation Assistant can be found in the Hootenanny User Interface Guide. 

This document provides background on how to use Hootenanny in a number of common use cases and work flows with a strong emphasis on the command line interface. Note that there are a number of undocumented features in Hootenanny that are referenced from the command line help. If you need more detailed information on that functionality please contact the developers. ( mailto:hootenanny.help@digitalglobe.com[hootenanny.help@digitalglobe.com] )

[[Installation]]
== Installation

Hootenanny can be installed on both CentOS/RHEL 6.5 and Ubuntu 12.04/14.04 operating systems.  See the <<hootInstall, Hootenanny Installation>> and <<hootDevGuide, Hootenanny Developer>> Guides for additional information. 

[[OldDocsConflation]]
== Conflation

[[UnifyingPoi]]
=== POI Conflation

Hootenanny provides the ability to perform point to point, i.e. POI to POI, conflation to merge input dataset geometry and attribution using the Unify approach. The _Unify_ POI to POI conflation code uses the Hootenanny JavaScript interface described in <<HootJavaScriptOverview>> 
to find, label and merge match candidate features. Because this code deals with a broader range of POI conditions, conflating POI 
datasets using Unify will invariably produce reviews that the user must manually confirm using the workflow described in the
<<hootUI>>, _Reviewing Conflicts_.  Further background on Unifying POI conflation can be found in <<hootalgo>>, _Unifying
POI Conflation_.

There are a number of configurable distance thresholds that can be specified if
you are comfortable making changes to JavaScript. Please see
`$HOOT_HOME/rules/PoiGeneric.js` to modify the tables found in the script. Any
changes made to `PoiGeneric.js` will apply to all jobs launched in the user
interface as well as command line.

To enable or change the POI conflation approach please see the following
configuration options:

* `match.creators` - <<match.creators>>
* `merger.creators` - <<merger.creators>>

For background on leveraging the Hootenanny JavaScript interface to call
functions using Node.js, see <<HootJavaScriptOverview>>.

[[Building-Conflation]]
=== Building Conflation

Building conflation supports multiple forms:

* One to one
* One to many
* Many to many

The attribute merging in the one to one case when each building is made up of a
single part is very simple and controlled through the typical
+tag.merger.default+ configuration setting.

If one of the buildings contains multiple parts, each with their own attributes such as building height, roof type, etc. The problem becomes more complex. Additional complexity is added when you're conflating one multi-part building with multiple multi-part buildings, or in the worse case, multiple multi-part buildings all being conflated into a single building.

Obviously, combining n records into a single record requires that some attributes are merged or dropped, but the question is, which ones? At this point in time the attribute merging occurs in two steps:

* Multi-part building tags are combined together incrementally to create a parent relation that minimizes conflicts with child buildings. A write up on the specifics will come in the future.
* Multiple buildings on each side of the pairwise merger are combined further into a single building with minimal conflicting tags.
* Two building groups are merged together using the +tag.merger.default+ merging method to create a new building. Only the building parts from the more complex building are kept.

The approach described above generally provides good results. There is no getting around it, the approach will drop information during conflation in some situations. As new business rules are defined we expect to revisit the above attribute merging approach to create something that addresses specific user concerns and needs. Please contact the developers if you have any specific needs or concerns.


[[POI-to-Polygon-Conflation]]
=== POI to Polygon Conflation
 
Hootenanny conflates POIs with both building and area polygons.  It uses the distance between the two features, name similarity, type similarity, and address similarity as the criteria for matching the features.  See <<hootalgo, Hootenanny - Algorithms>>, POI to Polygon Conflation section for specific algorithm details.

[[POI-to-Polygon-Configurable-Options]]
==== Configurable Options
  
See the User Guide Command Line Documentation section for all configuration options beginning with the text "poi.polygon".

[[River-Conflation]]
=== River Conflation

Rivers may be conflated using the Javascript generic conflation capability.  For more information on
generic conflation, see the related sections in this document.  See the algorithms documentation for 
more details on the algorithms and techniques used in river conflation.

==== Configurable Files

.*River Conflation Related Files*
[width="65%"]
|======
| *Location* | *Description*
| `conf/WaterwaySchema.json` | attribute schema definitions for river data
| `rules/LinearWaterway.js` | custom rule based model for river conflation
|======

==== Usage

River conflation can be done from the command line or the web user interface.  This section describes
how to conflate river data from the command line.  For details on how to do it in the web user interface,
see the associated section in the Hootenanny User Interface guide.  To conflate river data, a 
command similar to the following may be issued:

------
hoot conflate <river-dataset-1> <river-dataset-2> <output>
------

All of the settings that can be modified for river conflation exist in +conf/ConfigOptions.asciidoc++ and are read in by 
+rules/LinearWaterway.js+  Tweaking the settings can result in better conflation performance depending on the datasets being conflated.  
See the configuration options for details on the settings that may be modified (search for "waterway").

[[Feature-Review]]
=== Feature Review

During the conflation process if Hootenanny cannot determine with confidence the best way to 
conflate features, it will mark one or more features as needing a manual review by the user.  Below
are listed the possible solutions where Hootenanny may request a manual review from a user.

.*Feature Review Situations*
[width="100%"]
|======
| *Needs Review Message* | *Possible Causes* | *Potential User Actions to Take*
| Elements contain too many ways and the computational complexity is unreasonable. | A multiline string geometry was attempted to be conflated that had more lines in it than Hootenanny can conflate in a reasonable amount of time using its current algorithms. Currently, that is when both input sublines to conflate contain more than four lines, or if the sum of lines they contain is greater than seven. | Review this feature manually. It can only be automatically conflated by developing new conflation algorithms that can handle multilinestring input data of this size.
| Internal Error: Expected a matching subline, but got an empty match. Please report this to the developers. | An unexpected internal error occurred. Multiple matching line parts have caused extra difficulty during the line matching process. | Review this feature manually, and report this behavior to the Hootenanny development team for further examination.
| MultiLineString relations can only contain ways when matching sublines. | A input being conflated of geometry type multiline string contained an OSM feature type other than a way (nodes or relations). | Review this feature manually. This is invalid input data for Hootenanny conflation purposes and cannot be conflated automatically.
| Multiple overlapping way matches were found within one set of ways. | When attempting to conflate sublines from line inputs, multiple overlapping lines were found. | TODO
| No valid matching subline found. | When conflating two linear inputs, Hootenanny could not find a corresponding matching subline part in one of the inputs. | Review this feature manually, as Hootenanny can not determine automatically whether it matches any feature in the opposite input dataset.
| When matching sublines expected a multilinestring relation not a <osm feature type>. | When conflating linear features, Hootenanny expects all relations to be of the type multilinestring. | Review this feature manually. This is invalid input data for Hootenanny conflation purposes and cannot be conflated.
|======

[[Translation]]
== Translation

Translation is the process of both converting tabular GIS data, such as
Shapefiles, to the OSM format and schema. There are two main supported formats
for OSM data, +.osm+ , an XML format, and +.osm.pbf+ , a compressed binary
format. Discussions of OSM format reference either of these two data formats. 

By far the most complex portion of the translation process is the converting the
Shapefile's schema to the OSM schema. In many cases a one to one mapping can be
found due to the richness of the OSM schema, but finding the most appropriate mapping 
can be quite time consuming.  For example, one can spend days translating an obscure
local language to determine the column headings and values in the context of OSM or 
depending on their knowledge of Python/Javascript, create a custom translation value that 
provides a mapping between the two schemas in a significantly shorter duration of time.  

The following sections discuss high level issues associated with translating
files. For a more nuts and bolts discussion see the +--ogr2osm+ section.

[[JavaScript-Translation]]
=== JavaScript Translation

Hootenanny support translation files written in both Python and JavaScript (AKA
ECMA Script). The JavaScript engine used by Hootenanny is the engine integrated
with Qt. See the http://qt-project.org/doc/qt-4.7/ecmascript.html[Qt ECMA Script
Documentation] for details on which operations are supported.

[[Overview]]
==== Overview

[[Special-Operations]]
===== Special Operations

In addition to the operations exposed by Qt, the user also has access to:

*  +require+ - Require a JavaScript module provided by Hootenanny. The list of
   supported modules is still being defined.
*  +print+ - Print a line to stdout
*  +debug+ , +logDebug+ - Print debug text to stdout using the Hootenanny
   logging facilities. Each message will include date/time, filename, and line
   number. E.g. logs if the +--debug+ flag has been set on the command line.
*  +logInfo+ - Print information text to stdout using the Hootenanny logging
   facilities. Each message will include date/time, filename, and line number.
*  +warn+ , +logWarn+ - Print warning text to stdout using the Hootenanny
   logging facilities. Each message will include date/time, filename, and line
   number.
*  +logError+ - Print error text to stdout using the Hootenanny logging
   facilities. Each message will include date/time, filename, and line number.
*  +logFatal+ - Print fatal text to stdout using the Hootenanny logging
   facilities. Each message will include date/time, filename, and line number.


[[Functions-Called-by-Hootenanny]]
===== Functions Called by Hootenanny

There are several functions that may be called by Hootenanny:

*  +initialize+ - An optional method that gets called before any other methods.
*  +finalize+ - An optional method that gets called after all other methods have
   been completed. This can be useful if you want to print out statistics on the
   translation.
*  +translateToOgr+ - Required by the +--osm2ogr+ command to translate from OSM
   to a custom schema.
*  +translateToOsm+ - Required by the +--ogr2osm+ command to translate from a
   custom schema to the OSM schema. For backwards compatibility reasons
   +translateAttributes+ is also supported, but +translateToOsm+ is preferred.
*  +getDbSchema+ - Required by the +--osm2ogr+ command to get the custom schema
   that OSM data will be converted into.

[[Simple-Example]]
===== Simple Example
  

Below is about the simplest useful example that supports both +--ogr2osm+ and +--osm2ogr+ . The following sections go into details on how these function are used.

------
// an optional initialize function that gets called once before any 
// translateAttribute calls.
function initialize()
{
    // The print method simply prints the string representation to stdout
    print("Initializing.")
}

// an optional finalize function that gets called once after all
// translateAttribute calls.
function finalize()
{
    // the debug method prints to stdout when --debug has been specified on
    // the hoot command line. (DEBUG log level)
    debug("Finalizing.");
}

//
// A very simple function for translating NFDDv4's to OSM:
// - NAM column to OSM's name tag
// - TYP column to OSM's highway tag
// This is far from complete, but demonstrates the concepts.
//
function translateToOgr(tags, elementType, geometryType)
{
    var attrs = {};

    if ('name' in tags)
    {
        attrs['NAM'] = tags['name'];
    }

    attrs['TYP'] = 0;
    if (tags['highway'] == 'road')
    {
        attrs['TYP'] = 1;
    }
    else if (tags['highway'] == 'motorway')
    {
        attrs['TYP'] = 41;
    }

    return { attrs: attrs, tableName: "LAP030" };
}

//
// A very simple function for translating from OSM's schema to NFDDv4:
// - name tag to NFDDv4's NAM column
// - highway tag to NFDDv4's TYP column
// This is far from complete, but demonstrates the concepts.
//
function translateToOsm(attrs, layerName)
{
    tags = {};

    if (attrs['NAM'] != '')
    {
        tags['name'] = attrs['NAM']
    }
    if (attrs['TYP'] == 41)
    {
        tags['highway'] = 'motorway';
    }
    else
    {
        tags['highway'] = 'road';
    }

    return tags
}

//
// This returns a schema for a subset of the NFDDv4 LAP030 (road) columns.
//
function getDbSchema()
{
    var schema = [
        lap030 = {
            name: 'LAP030',
            geom: 'Line',
            columns: [
                {
                    name:'NAM',
                    type:'String'
                },
                { name:"TYP",
                  desc:"Thoroughfare Type" ,
                  optional:"O" ,
                  type:"enumeration",
                  enumerations:[
                     { name:"Unknown", value:"0" },
                     { name:"Road", value:"1" },
                     { name:"Motorway", value:"41" }
                  ] // End of Enumerations 
                 } // End of TYP
            ]
        }
    ]

    return schema;
}
------


[[JavaScript-to-OSM-Translation]]
==== JavaScript to OSM Translation
  

The +translateToOsm+ method takes two parameters:

*  +attrs+ - A associative array of attributes and values from the source record.
*  +layerName+ - The name of the layer being processed. In the case of a Database source it will be the table name. In the case of a file input it will be the full path to the file. Frequently the +layerName+ is useful in decoding the type of feature being processed.

_Note_: The +translateToOsm+ was previously called +translateAttributes+ . Either name will still work, but +translateToOsm+ is preferred. If both are specified then +translateToOsm+ will be used.

This method will be called after the +initialize+ method is called when translating from an OGR format to a OSM schema. For instance if you call:

------
hoot --ogr2osm tmp/SimpleExample.js myoutput.osm myinput1.shp myinput2.shp
------

The functions will be called in the following order:

.  +initialize+ 

.  +translateToOsm+ - This will be called once for every feature in myinput1.shp

.  +translateToOsm+ - This will be called once for every feature in myinput2.shp

.  +finalize+ 


[[Table-Based-Translation]]
===== Table Based Translation
  

For more advanced translations it may make sense to define a simple set of tables and use those tables to translate values. An example is below:

------
// create a table of nfdd biased rules.
var nfddBiased = [
    { condition:"attrs['SBB'] == '995'", consequence:"tags['bridge'] = 'yes'" }
];

// build a one to one translation table.
var one2one = [
    ['ROC', '1',    'surface',  'ground'],
    ['ROC', '2',    'surface',  'unimproved'],
    ['WTC', '1',    'all_weather', 'yes'],
    ['WTC', '2',    'all_weather', 'fair']
];

// build a more efficient lookup
var lookup = {}
for (var r in one2one)
{
    var row = one2one[r];
    if (!(row[0] in lookup))
    {
        lookup[row[0]] = {}
    }

    lookup[row[0]][row[1]] = [row[2], row[3]];
}

// A translateAttributes method that is very similar to the python translate 
// attributes
function translateToOsm(attrs, layerName) 
{ 
    var tags = {};

    for (var col in attrs)
    {
        var value = attrs[col];
        if (col in lookup)
        {
            if (value in lookup[col])
            {
                row = lookup[col][value];
                tags[row[0]] = row[1];
            }
            else
            {
                throw "Lookup value not found for column. (" + col + "=" + value + ")";
            }
        }
        else
        {
            for (var bi in nfddBiased)
            {
                print(attrs['SBB']);
                print(nfddBiased[bi].condition);
                print(eval(nfddBiased[bi].condition));
                print(nfddBiased[bi].consequence);
                if (eval(nfddBiased[bi].condition))
                {
                    print("Condition true.");
                    eval(nfddBiased[bi].consequence);
                }
            }
        }
    }
    return tags;
}
------


[[OSM-to-OGR-Translation]]
==== OSM to OGR Translation
  

Using JavaScript translation files it is now possible to convert from OSM to more typical tabular geospatial formats such as Shapefile or FileGDB. In order to convert to these formats some information will likely be lost and these translation files define which attributes will be carried across and how they'll be put into tables/layers.

The necessary functionality is accessed via two methods, +getDbSchema+ and +translateToOsm+ . Both methods are required.

The +getDbSchema+ method takes no arguments and returns a complex schema data structure that is described in theDB Schemasection.

The +translateToOsm+ method takes three arguments and returns an associative array values.
Arguments:

*  +tags+ - A associative array of tag key/value pairs from the source element/feature.
*  +elementType+ - The OSM element type being passed in. This is one of "node", "way", or "relation". See the OSM data model for more information.
*  +geometryType+ - The geometry type of the element being passed in. This is one of "Point", "Line", "Area" or "Collection". The value is determined based on both the element type and the tags on a given feature.

Returns:

*  +undefined+ if the feature should be dropped, or a single associative array with the following keys:
*  +attrs+ - An associative array of attributes where the key is the column name and the value is the cell's value. The cell's value does not need to be in the same data type as specified by the schema, but must be convertible to that data type. For instance returns a string zero ( +"0"+ ) and integer zero ( +0+ ) are both acceptable for an integer field. The attrs must be consistent with the table schema defined for the given +tableName+ .
*  +tableName+ - A string value the determines the table/layer that the feature will be inserted into. This must be one of the tables defined in the DB schema.


The methods will be called after the +initialize+ method is called when translating from an OGR format to a OSM schema. For instance if you call:

------
hoot --osm2ogr tmp/SimpleExample.js myinput.osm myoutput.shp
------

The functions will be called in the following order:

.  +initialize+ 

.  +getDbSchema+ 

.  +translateToOgr+ - This will be called once for every element in myinput.osm that has at least one non-metadata tag. The metadata tags are defined in +$HOOT_HOME/conf/MetadataSchema.json+ 

.  +finalize+ 

This is most commonly accessed through the +--osm2ogr+ command.


[[DB-Schema]]
===== DB Schema
  

Hootenanny supports converting OSM data into multiple layers where each layer has its own output schema including data types and column names.

The DB schema result is structured as follows:

------
// The top level schema is always defined as an array of table schemas
schema = [
  // each table is an associative array of key/values
  {
    // required name of the layer. This is the layer name that will be created.
    name: "ROAD_TABLE",
    // required geometry type for a table. Options are Point, Line and Polygon
    geom: "Line",
    // required array of columns in the table.
    columns: [
      {
        // required name of the column
        name: "NAM",
        // required type of the column. 
        // Options are listed in "Supported output data types" below.
        type: "string",
        // Optional defValue field. If the column isn't populated in attrs then
        // this defValue will be used. If it isn't specified then the column
        // must always be specified in attrs.
        defValue: '',
        // Optional length field. If the column isn't populated then the default
        // field size is used as defined by OGR. If it is populated then the 
        // value will be used as the field width.
        length: 255
      },
      // another column
      { name: "TYP", type: "enumeration",
        // enumerated values
        enumerations: [
          { value: 0 }, 
          { value: 1 }
        ]
      }
    ]
  }
  // any number of tables can be defined here.
];
------

Supported output data types:

*  +string+ - A variable length string.
*  +enumeration+ - A 32bit signed integer with specific acceptable enumerated values.
*  +double+ or +real+ - 64bit float
*  +integer+ or +long integer+ - Aliased to +enumeration+, but it doesn't require an +enumerations+ array.

The numeric data types support +minimum+ and +maximum+ . By default +minimum+ and +maximum+ are disabled. If min/max values are specified or an enumeration table is populated then Hootenanny will validate all output data before it is written. The following rules are used to determine if a value is valid:

* If the enumeration table is present ( +enumeration+ type only) then a value is valid. If the value is in the enumeration table then min/max bounds are ignored.
* If +maximum+ is specified then the value is invalid if it is greater than maximum.
* If +minimum+ is specified then the value is invalid if it is less than minimum.


[[File-Formats]]
==== File Formats
  
For the translation operations (and several others) Hootenanny utilizes the well known GDAL/OGR libraries. These libraries support a number of file formats including Shapefile, FileGDB, GeoJSON, PostGIS, etc. While not every format has been tested, many will work with Hootenanny without any modification. Others, such as FileGDB, may require a specially compiled version of GDAL. Please see the GDAL documentation and talk to your administrator for details.

Below are a discussion of some special handling situations when reading and writing to specific formats.


[[Shapefile]]
===== Shapefile
  
When writing shapefiles a new directory will be created with the basename of the specified path and the new layers will be created within that directory. For example:

------
hoot --osm2ogr translations/MyTranslation.js input.osm output.shp
------

The above command will create a new directory called +output+ and the layers specified in the +translations/MyTranslation.js+ schema will be created as +output/<your layer name>.shp+ .


[[CSV]]
===== CSV
  

CSV files are created using the OGR CSV driver and will contain an associated +.csvt+ file that contains the column types. If you're exporting points then you will get an X/Y column prepended onto your data. If you're exporting any other geometry type then you will get a WKT column prepended that contains the Well Known Text representation of your data. If you would like to read from a CSV you must first create a VRT file as described in the OGR CSV documentation. E.g.

Creating a new CSV file:

------
hoot --osm2ogr translations/Poi.js test-files/conflate/unified/AllDataTypesA.osm foo.csv
------

This uses a simple translation script ( +Poi.js+ ) that exports POI data and its associated tags. If you would then like to read that data create a new +.vrt+ file named +foo.vrt+ that contains the following:

------
<OGRVRTDataSource>
    <OGRVRTLayer name="foo">
        <SrcDataSource>foo.csv</SrcDataSource>
        <GeometryType>wkbPoint</GeometryType>
        <LayerSRS>WGS84</LayerSRS>
        <GeometryField encoding="PointFromColumns" x="X" y="Y"/>
    </OGRVRTLayer>
</OGRVRTDataSource>
------

Then to convert the file back into a .osm file run:

------
hoot --ogr2osm translations/Poi.js ConvertedBack.osm foo.vrt
------


[[Buildings-Translation]]
=== Buildings Translation
  

In the simplest case a building is a way tagged with +building=yes+ . However, when it comes to 3D features buildings can get dramatically more complex. For a thorough discussion of Buildings and how they're mapped see the link:$$http://wiki.openstreetmap.org/wiki/Simple_3D_Buildings$$[OSM wiki page on Simple 3D Buildings] .


[[Translating-Building-Parts]]
==== Translating Building Parts
  

Some Shapefiles contain buildings that are mapped out as independent parts. Where each part refers to the roof type and height of a portion of the building. E.g. The Capital building might be mapped out as one large, low flat roof record and a second tall domed roof record. This provides for very rich data, but also a complex representation in OSM. Fortunately Hootenanny handles most of the heavy lifting for you.

To translate complex building parts simply translate them in the same way you would translate any other building. By default Hootenanny will then search through all the buildings and look for buildings that appear to be part of the same structure. If they're part of the same structure then a complex building will be created for you automatically. The complex buildings will take the form specified in the link:$$http://wiki.openstreetmap.org/wiki/Simple_3D_Buildings$$[Simple 3D Buildings] specification. The following section gives a specific example.


[[Complex-Building-Example]]
===== Complex Building Example
  
.Example of a Complex Building

image::user/images/image1348.png[]

In the above image there are three buildings; 123, 124, and 125. Building 123 is broken into two parts, a long rectangular section that is marked as a gabled roof and a squarish section that is marked with a flat roof. In a Shapefile that may look like the following:

|======
| name | roof_type 
| 123 | gabled 
| 123 | flat 
| 124 | gabled 
| 125 | gabled 
|======

Using an abbreviated OSM JSON representation the resulting OSM data would be:

------
{ "type": "way", "id": 1, "tags": { "building": "yes", "addr:housenumber": "123", "building:roof:shape": "gabled" } }
{ "type": "way", "id": 2, "tags": { "building": "yes", "addr:housenumber": "123", "building:roof:shape": "flat" } }
{ "type": "way", "id": 3, "tags": { "building": "yes", "addr:housenumber": "124", "building:roof:shape": "gabled" } }
{ "type": "way", "id": 4, "tags": { "building": "yes", "addr:housenumber": "125", "building:roof:shape": "gabled" } }
------

Hootenanny will automatically detect that the two 123 buildings are part of the same building. This is done by asking the following questions:

* Do the two building share at least two consecutive nodes (share an edge) or does one building completely contain the other building?
* Do the non-part specific attributes of buildings match very closesly? (E.g. Are the addresses the same? Are the names the same? Ignore any differences in height or roof shape.)

If these two questions answer yes, then the building parts are grouped together. An arbitrary number of building parts may be grouped together in this way to create a larger building. Once the building parts are grouped some new elements are added to the map to represent the building parts as shown in the following OSM JSON snippet.

------
{ "type": "way", "id": 1, "tags": { "building:part": "yes", "building:roof:shape": "gabled" } }
{ "type": "way", "id": 2, "tags": { "building:part": "yes", "building:roof:shape": "flat" } }
{ "type": "way", "id": 3, "tags": { "building": "yes", "addr:housenumber": "124", "building:roof:shape": "gabled" } }
{ "type": "way", "id": 4, "tags": { "building": "yes", "addr:housenumber": "125", "building:roof:shape": "gabled" } }
{ "type": "way", "id": 5, "tags": { "building": "yes", "addr:housenumber": "125" } }
{ "type": "relation", "id": 1, "tags": { "type": "building", "building": "yes", "addr:housenumber": "123" }, 
    "members": [ 
        { "type": "way", "ref": 1, "role": "part" }
        { "type": "way", "ref": 2, "role": "part" }
        { "type": "way", "ref": 5, "role": "outline" } ] }
------

The astute reader may notice that a new way was created during this process. The new way, 5, is an outline of the entire building. This is done as part of the spec to be certain that older rendering engines don't ignore the complex building. Whenever building outlines are encountered by Hootenanny they are ignored and the more complex representation is used. However, Hootenanny will still generate building outlines. The building outline will always represent the union of all the building parts.


[[Disabling-Complex-Buildings]]
===== Disabling Complex Buildings
  

By default the +ogr2osm.ops+ parameter is set to +hoot::MergeNearbyNodes;hoot::BuildingPartMergeOp+ . If you would like to disable the automatic construction of complex buildings from the individual parts then simply remove +hoot::BuildingPartMergeOp+ from the +ogr2osm.ops+ parameter. For example:

------
hoot ogr2osm -D "ogr2osm.ops=hoot::MergeNearbyNodes" MyTranslation MyOutput.osm MyInput.shp
------

[[Common-Use-Cases]]
== Common Conflation Use Cases
  

The following sections describe some common use cases and how to approach them using Hootenanny.


[[Conflate-Two-Shapefiles]]
=== Conflate Two Shapefiles
  

The following subsections describe how to do the following steps:

. Prepare the input for translation

. Translate the Shapefiles into .osm files

. Conflate the Data

. Convert the conflated .osm data back to Shapefile

We'll be using files from the http://www.census.gov/geo/www/tiger/tgrshp2012/tgrshp2012.html[US Census Tiger] data and http://dcgis.dc.gov[DC GIS] 

* Tiger Roads - link:$$ftp://ftp2.census.gov/geo/tiger/TIGER2012/ROADS/tl_2012_11001_roads.zip$$[ftp://ftp2.census.gov/geo/tiger/TIGER2012/ROADS/tl_2012_11001_roads.zip] 
* DC GIS Roads - http://dcatlas.dcgis.dc.gov/catalog/download.asp?downloadID=88&downloadTYPE=ESRI[http://dcatlas.dcgis.dc.gov/catalog/download.asp?downloadID=88&downloadTYPE=ESRI] 


[[Prepare-the-Shapefiles]]
==== Prepare the Shapefiles
  

First validate that your input shapefiles are both Line String (AKA Polyline) shapefiles. This is easily done with +ogrinfo+:

------
$ ogrinfo -so tl_2010_12009_roads.shp tl_2010_12009_roads
INFO: Open of `tl_2010_12009_roads.shp'
      using driver `ESRI Shapefile' successful.

Layer name: tl_2010_12009_roads
Geometry: Line String
Feature Count: 17131
Extent: (-80.967774, 27.822067) - (-80.448353, 28.791396)
Layer SRS WKT:
GEOGCS["GCS_North_American_1983",
    DATUM["North_American_Datum_1983",
        SPHEROID["GRS_1980",6378137,298.257222101]],
    PRIMEM["Greenwich",0],
    UNIT["Degree",0.017453292519943295]]
STATEFP: String (2.0)
COUNTYFP: String (3.0)
LINEARID: String (22.0)
FULLNAME: String (100.0)
RTTYP: String (1.0)
MTFCC: String (5.0)
------


[[Translate-the-Shapefiles]]
==== Translate the Shapefiles
  

Hootenanny provides a link:$$User_-_--ogr2osm.html$$[--ogr2osm] operation to translate and convert shapefiles into OSM files. If the projection is available for the Shapefile the input will be automatically reprojected to WGS84 during the process. If you do a good job of translating the input data into the OSM schema then Hootenanny will conflate the attributes on your features as well as the geometries. If you do not translate the data properly then you'll still get a result, but it may not be desirable.


[[Crummy-Translation]]
===== Crummy Translation
  

The following translation code will always work for roads, but drops all the attribution on the input file.

------
#!/bin/python

def translateAttributes(attrs, layerName):
    if not attrs: return

    return {'highway':'road'}
------


[[Better-Translation]]
===== Better Translation
  

The following translation will work well with the tiger data.

------
#!/bin/python

def translateAttributes(attrs, layerName):
    if not attrs: return

    tags = {}

    # 95% CE in meters
    tags['accuracy'] = '10'

    if 'FULLNAME' in attrs:
        name = attrs['FULLNAME']
        if name != 'NULL' and name != '':
            tags['name'] = name

    if 'MTFCC' in attrs:
        mtfcc = attrs['MTFCC']
        if mtfcc == 'S1100':
            tags['highway'] = 'primary'
        if mtfcc == 'S1200':
            tags['highway'] = 'secondary'
        if mtfcc == 'S1400':
            tags['highway'] = 'unclassified'
        if mtfcc == 'S1500':
            tags['highway'] = 'track'
            tags['surface'] = 'unpaved'
        if mtfcc == 'S1630':
            tags['highway'] = 'road'
        if mtfcc == 'S1640':
            tags['highway'] = 'service'
        if mtfcc == 'S1710':
            tags['highway'] = 'path'
            tags['foot'] = 'designated'
        if mtfcc == 'S1720':
            tags['highway'] = 'steps'
        if mtfcc == 'S1730':
            tags['highway'] = 'service'
        if mtfcc == 'S1750':
            tags['highway'] = 'road'
        if mtfcc == 'S1780':
            tags['highway'] = 'service'
            tags['service'] = 'parking_aisle'
        if mtfcc == 'S1820':
            tags['highway'] = 'path'
            tags['bicycle'] = 'designated'
        if mtfcc == 'S1830':
            tags['highway'] = 'path'
            tags['horse'] = 'designated'

    return tags
------

To run the tiger translation put the above code in a file named +translations/TigerRoads.py+ and run the following:

------
hoot --ogr2osm TigerRoads tmp/dc-roads/tiger.osm tmp/dc-roads/tl_2012_11001_roads.shp
------

The following translation will work OK with the DC data.

------
#!/bin/python

def translateAttributes(attrs, layerName):
    if not attrs: return

    tags = {}

    # 95% CE in meters
    tags['accuracy'] = '15'

    name = ''
    if 'REGISTERED' in attrs:
        name = attrs['REGISTERED']
    if 'STREETTYPE' in attrs:
        name += attrs['STREETTYPE']
    if name != '':
        tags['name'] = name

    if 'SEGMENTTYP' in attrs:
        t = attrs['SEGMENTTYP']
        if t == '1' or t == '3':
            tags['highway'] = 'motorway'
        else:
            tags['highway'] = 'road'

    # There is also a one way attribute in the data, but given the difficulty 
    # in determining which way it is often left out of the mapping.

    return tags
------

To run the DC GIS translation put the above code in a file named +translations/DcRoads.py+ and run the following:

------
hoot --ogr2osm DcRoads tmp/dc-roads/dcgis.osm tmp/dc-roads/Streets4326.shp
------


[[Conflate-the-Data]]
==== Conflate the Data
  

If you're just doing this for fun, then you probably want to crop your data down to something that runs quickly before conflating.

------
hoot --crop-map tmp/dc-roads/dcgis.osm tmp/dc-roads/dcgis-cropped.osm "-77.0551,38.8845,-77.0281,38.9031" 
hoot --crop-map tmp/dc-roads/tiger.osm tmp/dc-roads/tiger-cropped.osm "-77.0551,38.8845,-77.0281,38.9031" 
------

All the hard work is done. Now we let the computer do the work. If you're using the whole DC data set, go get a cup of coffee.

------
hoot --conflate tmp/dc-roads/dcgis-cropped.osm tmp/dc-roads/tiger-cropped.osm tmp/dc-roads/output.osm
------


[[Convert-Back-to-Shapefile]]
==== Convert Back to Shapefile
  

Now we can convert the final result back into a Shapefile.

------
hoot --osm2shp "name,highway,surface,foot,horse,bicycle" tmp/dc-roads/output.osm tmp/dc-roads/output.shp
------


[[Snap-GPS-Tracks-to-Roads]]
=== Snap GPS Tracks to Roads
  

. Create a translation file for "translating" your GPS tracks. This typically just adds the accuracy field. E.g. +accuracy=5+ 

. Convert your GPX file into an OSM file where each track is now a way.

------
hoot --ogr2osm GpsTrack tmp/MyTracks.osm "$HOME/MyTracks.gpx;tracks" 
------
+

. Use the special track snapping conflation manipulation to snap your tracks to an existing road network and convert to Shapefile.
------
hoot conflate -D conflator.manipulators=hoot::WaySnapMerger HighQualityRoads.osm tmp/MyTracks.osm tmp/MySnappedTracks.osm
hoot osm2shp "hoot:max:movement,hoot:mean:movement,hoot:score,name,foot" tmp/MySnappedTracks.osm tmp/MySnappedTracks.shp
------



[[Maintaining-per-node-attributes]]
==== Maintaining per node attributes
  

If you have node attributes that you want to keep you can use the +hoot::PointsToTracksOp+ operation to join the nodes after translation. This requires two fields on each node:

*  +hoot:track:id+ - The id of the track that the node belongs to. The id is simply treated as a string. Nodes with like ids will be grouped together.
*  +hoot:track:seq+ - The sequence of the nodes within the track (way). This is treated as a string and sorted as a string where the smallest value is at the beginning of the track. Be certain to avoid problems with integers during translation. E.g. "13", "112" will not sort properly, but "013", "112" will sort properly. It is also recommended to use +hoot::MergeNearbyNodes+ as a poor man's line simplification to speed the process up a bit. If this causes problems with your data you can safely drop it.

The command used with a GPX input file is:

------
hoot ogr2osm  -D "ogr2osm.ops=hoot::MergeNearbyNodes;hoot::PointsToTracksOp" GpsTrack tmp/MyTracks.osm "$HOME/MyTracks.gpx;track_points" 
------

An example translation file is:

------
#!/bin/python

def translateAttributes(attrs, layerName):
    if not attrs: return

    tags = attrs
    tags['accuracy'] = '5'
    tags['highway'] = 'road'
    if 'track_fid' in attrs:
        tags['hoot:track:id'] = attrs['track_fid']
        tags['hoot:track:seq'] = "%09d" % int(attrs['track_seg_point_id'])

    return tags
------

*Special Rule* If all the nodes in a track have the same +highway=*+ setting then the highway attribute will be moved from the node to the way.


[[Add-NSG-TLM-Symbology-to-a-FileGeodatabase]]
=== Add NSG TLM Symbology to a FileGeodatabase

==== Overview
ESRI ArcMap can use Visual Representation rules to display symbology. Hootenanny is able to export Topographic Data Store (TDS) compliant data in a FileGeodatabase that is able to have default symbology applied to it. The command line procedure to create default symbology is as follows.

References:

* http://resources.arcgis.com/en/help/main/10.2/index.html#/What_are_representations/00s50000004m000000/
* http://resources.arcgis.com/en/help/main/10.2/index.html#/What_are_visual_specifications/0103000001w9000000/

==== Requirements
The main requirement is access to a copy of ESRI ArcGIS with the following:

* ArcGIS Standard or ArcGIS Desktop license
* Production Mapping Extension & license
* Defense Mapping Extension & license

==== Process:

Get an empty TDS template FileGeodatabase::
* From the ArcGIS Defence Mapping Extension install location.
+
----
C:\Program Files\ArcGIS\EsriDefenseMapping\Desktop10.2\Tds\Local\Schema\Gdb\LTDS_4_0.zip+
----
+
* Unpack this Zip file and copy the "LTDS_4_0.gdb" File GeoDatabase to your Hootenanny working directory.

Run Hootenanny and add your data to the template File GeoDatabase::
* Add the "Append Data" flag: +ogr.append.data+
* Add the template File GeoDatabase to write to.
+
----
hoot osm2ogr -D ogr.append.data="true" $HOOT_HOME/translations/TDS.js your_data.osm LTDS_4_0.gdb
----

Transfer the LTDS_4_0.gdb to the machine that has ESRI ArcGIS installed::
* Place it in a convenient location


Set the "Product Library" in ArcMap::
Reference: http://resources.arcgis.com/en/help/main/10.2/index.html#//0103000001p0000000
* Copy "C:\Program Files\ArcGIS\EsriDefenseMapping\Desktop10.2\Tds\Local\Product Library\LTDS_4_0_Product_Library.zip" to where you saved the Hoot File GeoDatabase
* Unzip "LTDS_4_0_Product_Library.zip" to get LTDS_4_0_Product_Library.gdb"
* Open ArcMap:
** Click on "Customize->Production->Product Library"
** Right Click on "Product Library"
** Click on "Select Product Library"
** Navigate to wherever you saved the "LTDS_4_0_Product_Library.gdb" and select it.


Calculate the Visual Specifications::
Reference: http://resources.arcgis.com/en/help/main/10.2/index.html#/Calculate_Visual_Specifications/01090000001w000000/
* Open ArcCatalog
* Run "Toolboxes->System Toolboxes->Production Mapping Toolbox->Symbology->Calculate Visual Specifications":
** *Input Features* Browse to where the Hootenanny File GeoDatabase is saved and select all of the features inside the "LTDS" feature dataset
** *Visual Specification Workspace* Browse to and select "C:\Program Files\ArcGIS\EsriDefenseMapping\Desktop10.2\Tds\Local\Cartography\Symbology\LTDS_4_0_NSG_Visual_Specification.mdb"
** Select "LTDS_NSG::50K" for 50K TLM symbology or "LTDS_100K::100K" for a 100K TLM symbology.
** Click on "OK"
** Wait for it to finish. It will take a while.

View the Default Symbology::
* Open ArcMap
* Add the +LTDS_4_0.gdb+ dataset



