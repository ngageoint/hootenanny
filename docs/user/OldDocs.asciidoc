
== Using Hootenanny

Hootenanny can be accessed from both command line and via a simple web user interface (Hootenanny-iD) built on top of the link:$$https://www.openstreetmap.org/edit?editor=id$$[OpenStreetMap iD] Editor.  At its core, Hootenanny  provides a set of tools for manipulating and conflating vector data as well as translating data between various out of the box supported reference tag schemas such as OSM, TDS, UTP, MGCP.  As an extension to the existing translation capabilities within Hootenanny, the Hootenanny-iD web interface provides a simple translation tool known as the _Translation Assistant_ that will walk users through a custom translation of any customer data format into a supported tag schema.  Additional background on the Hootenanny-iD interface and Translation Assistant can be found in the Hootenanny User Interface Guide.

This document provides background on how to use Hootenanny in a number of common use cases and work flows with a strong emphasis on the command line interface. Note that there are a number of undocumented features in Hootenanny that are referenced from the command line help. If you need more detailed information on that functionality please contact the developers at https://github.com/ngageoint/hootenanny.

[[Installation]]
== Installation

See the <<hootInstall, Hootenanny Installation>> and <<hootDevGuide, Hootenanny Developer>> Guides for additional information.

[[OldDocsConflation]]
== Conflation Supported Feature Types

=== Road Conflation

<<hootalgo, RoadConflation>>

==== Conflation Road Algorithms

===== 2nd Generation (aka Unifying)

<<hootalgo, UnifyingConflation>>

===== Network

<<hootalgo, NetworkConflation>>

==== Configuration

Road Conflation can further be customized with the highway.* configuration options. See the "Configuration Options" section of User Guide 
for more detail.

[[PoiToPoi]]
=== POI Conflation

Hootenanny provides the ability to perform point to point, i.e. POI to POI, conflation to merge input dataset geometry and attribution using the Unify approach. The _Unify_ POI to POI conflation code uses the Hootenanny JavaScript interface described in <<HootJavaScriptOverview>>
to find, label and merge match candidate features. Because this code deals with a broader range of POI conditions, conflating POI
datasets using Unify will invariably produce reviews that the user must manually confirm using the workflow described in the
<<hootUI>>, _Reviewing Conflicts_.  Further background on Unifying POI conflation can be found in <<hootalgo>>, _Unifying
POI Conflation_. For more details on the POI to POI conflation algorithm, see <<hootalgo, PoiToPoi>>

There are a number of configurable distance thresholds that can be specified if
you are comfortable making changes to JavaScript. Please see
`$HOOT_HOME/rules/Poi.js` to modify the tables found in the script. Any
changes made to `Poi.js` will apply to all jobs launched in the user
interface as well as command line.

To enable or change the POI conflation approach please see the following
configuration options:

* `match.creators` - <<match.creators>>
* `merger.creators` - <<merger.creators>>

For background on leveraging the Hootenanny JavaScript interface to call functions using Node.js, see <<HootJavaScriptOverview>>.

[[Building-Conflation]]
=== Building Conflation

<<hootalgo, BuildingConflation>>

[[PoiToPolygonConflation]]
=== POI to Polygon Conflation

Hootenanny conflates POIs with both building and area polygons.  It uses the following criteria for matching features: distance between
the two features, name similarity, type similarity, address similarity, and phone number similarity.  See
<<hootalgo, PoiToPolygonConflation>> for specific algorithm details.

[[PoiToPolygonConfigurableOptions]]
==== Configurable Options

POI to Polygon Conflation can further be customized with the poi.polygon.* configuration options. See the "Configuration Options" section of 
User Guide for more detail.

[[PoiToPolygonStatistics]]
==== Statistics

Conflation statistics for POI to Polygon Conflation are available from the command line with the `--stats` option as well as in the User 
Interface, the same as with all other types of conflation.  Note that POIs conflatable with polygons have a different definition than those 
conflatable with other POIs, which is less strict.  Therefore, POIs conflatable with polygons are a superset of POIs conflatable with other 
POIs.  Likewise, polygons are a superset of buildings and also include features such as parks, parking lots, etc.  See the Feature Definitions 
section <<hootalgo, Hootenanny - Algorithms>> for POI and polygon definition details.

[[PoiToPolygonUseCases]]
==== Use Cases

===== Keep Closest Matches Only

By default, POI to Polygon uses a fairly large search radius to try consider as many things possible that might need to be matched together.
If the following are true, you may want to tweak POI to Polygon conflation to keep the closest distance matches only:

1) You are fairly confident in your geometries across both layers.
2) You have mostly generic type information on your features.
3) You are trying to generate fewer reviews at the expense of possibly missing some matches.

In this situation you could run with the following option set:

poi.polygon.keep.closest.matches.only=true - When a feature is matched to more than one other features, only the match with smallest distance between features is kept, despite the search radius
poi.polygon.additional.search.distance=0 - This reduces the overall search radius, thus reducing reviews (possible also matches) and increasing
runtime performance.
poi.polygon.match.threshold=1 - This will prevent reviews from being presented completely.

[[AreaToAreaConflation]]
=== Area to Area Conflation

Hootenanny makes Area to Area Conflation available, which is turned on by default when using command line conflation.  Hootenanny
defines an area as a non-building polygon that possesses the OSM `area=yes` tag or equivalent.  Examples: parks, parking lots.
To read more information on Area to Area Conflation, see <<hootalgo, AreaToAreaConflation>>.

[[River-Conflation]]
=== River Conflation

Rivers may be conflated using the Javascript generic conflation capability.  For more information on generic conflation, see the
related sections in this document.  See the algorithms documentation for more details on the algorithms and techniques used in
this conflation.

==== Configurable Files

.*River Conflation Related Files*
[width="65%"]
|======
| *Location* | *Description*
| `conf/schema/WaterwaySchema.json` | attribute schema definitions
| `rules/LinearWaterway.js` | custom rule based conflation model
|======

==== Usage

River conflation can be done from the command line or the web user interface.  This section describes how to conflate river data
from the command line.  For details on how to do it in the web user interface, see the associated section in the Hootenanny User
Interface guide.  To conflate river data, a command similar to the following may be issued:

------
hoot conflate <river-dataset-1> <river-dataset-2> <output>
------

All of the settings that can be modified for river conflation exist in +conf/core/ConfigOptions.asciidoc+.  Tweaking the settings can
result in better conflation performance depending on the datasets being conflated. See the "Configuration Options" section of User Guide for 
more detail.

[[Power-Line-Conflation]]
=== Power Line Conflation

Power lines may be conflated using the Javascript generic conflation capability.  For more information on generic conflation, see the
related sections in this document.  See the algorithms documentation for more details on the algorithms and techniques used in
this conflation.

==== Configurable Files

.*Power Line Conflation Related Files*
[width="65%"]
|======
| *Location* | *Description*
| `conf/schema/power.json` | attribute schema definitions
| `rules/PowerLine.js` | custom rule based conflation model
|======

==== Usage

Power line conflation can be done from the command line or the web user interface.  Conflating in both environments is similar as described
in the above River Conflation section.  Railway Conflation can further be customized with the power.line.* configuration options. See the 
"Configuration Options" section of User Guide for more detail.

[[Feature-Review]]
=== Feature Review

During the conflation process if Hootenanny cannot determine with confidence the best way to
conflate features, it will mark one or more features as needing a manual review by the user.  Below
are listed the possible solutions where Hootenanny may request a manual review from a user.

.*Feature Review Situations*
[width="100%"]
|======
| *Needs Review Message* | *Possible Causes* | *Potential User Actions to Take*
| Elements contain too many ways and the computational complexity is unreasonable. | A multiline string geometry was attempted to be conflated that had more lines in it than Hootenanny can conflate in a reasonable amount of time using its current algorithms. Currently, that is when both input sublines to conflate contain more than four lines, or if the sum of lines they contain is greater than seven. | Review this feature manually. It can only be automatically conflated by developing new conflation algorithms that can handle multilinestring input data of this size.
| Internal Error: Expected a matching subline, but got an empty match. Please report this to the developers. | An unexpected internal error occurred. Multiple matching line parts have caused extra difficulty during the line matching process. | Review this feature manually, and report this behavior to the Hootenanny development team for further examination.
| MultiLineString relations can only contain ways when matching sublines. | A input being conflated of geometry type multiline string contained an OSM feature type other than a way (nodes or relations). | Review this feature manually. This is invalid input data for Hootenanny conflation purposes and cannot be conflated automatically.
| Multiple overlapping way matches were found within one set of ways. | When attempting to conflate sublines from line inputs, multiple overlapping lines were found. | TODO
| No valid matching subline found. | When conflating two linear inputs, Hootenanny could not find a corresponding matching subline part in one of the inputs. | Review this feature manually, as Hootenanny can not determine automatically whether it matches any feature in the opposite input dataset.
|======

[[RailwayConflation]]
=== Railway Conflation

Railways may be conflated using the Javascript generic conflation capability.  For more information on generic conflation, see the
related sections in this document.  See the algorithms documentation for more details on the algorithms and techniques used in
this conflation.

==== Configurable Files

.*Railway Conflation Related Files*
[width="65%"]
|======
| *Location* | *Description*
| `conf/schema/railway.json` | attribute schema definitions
| `rules/Railway.js` | custom rule based conflation model
|======

Railway Conflation can further be customized with the railway.* configuration options. See the "Configuration Options" section of User Guide 
for more detail.

[[ConflationFeatureAccuracyHandling]]
== Conflation Feature Accuracy Handling

Feature accuracy is measure of the confidence in how accurately a feature's geospatial coordinates are. Feature accuracy values affect the
accuracy of conflated output data. Feature circular error (accuracy) can be processed in one of two ways: 1) on a feature by feature basis manually or 2) use Hootenanny's default value to assign it to all features. 

For 1), you can manually assign the  either the `circular:error` or `accuracy` tag with your CE value to individual features and Hootenanny will read in the values.

For 2), the default Hootenanny CE value is 15m and is controlled by the `circular.error.default.value` config option. If your feature data has neither the `circular:error` or `accuracy` tags present, hoot will use that value for all features. If you wanted to change the global CE value used for a conflate job to 5.0m, for example, you could do something like this for a conflate job:

`hoot conflate -D conflate.pre.ops++="hoot::SetTagValueVisitor" -D set.tag.value.visitor.keys="error:circular" -D set.tag.value.visitor.values=5.0 input1.osm input2.osm out.osm`

That will assign a CE of 5m to all features just before they are conflated.

If you just wanted to update your data with CE tags without conflating, you could just do something like:

`hoot convert -D convert.ops="hoot::SetTagValueVisitor"  -D set.tag.value.visitor.keys="error:circular" -D set.tag.value.visitor.values=5.0 -D writer.include.circular.error.tags=true input.osm out.osm`

You can also strictly control the search radius of the conflation routines if you don't want to use CE tags, which may be useful if you're not confident in your CE values and don't want to modify the data. The `search.radius.*` configuration options allow for controlling the conflate search radius. For example, if you wanted to conflate all features with a search radius of 25m:

`hoot conflate -D search.radius=25 input1.osm input2.osm out.osm`

Or if you just wanted to conflate buildings at a 25m radius:

`hoot conflate -D search.radius.building=25 -D match.creators="hoot::BuildingMatchCreator" -D merger.creators="hoot::BuildingMergerCreator" input1.osm input2.osm out.osm`

[[Translation]]
== Translation

Translation is the process of both converting tabular GIS data, such as
Shapefiles, to the OSM format and schema. There are two main supported formats
for OSM data, +.osm+ , an XML format, and +.osm.pbf+ , a compressed binary
format. Discussions of OSM format reference either of these two data formats.

By far the most complex portion of the translation process is the converting the
Shapefile's schema to the OSM schema. In many cases a one to one mapping can be
found due to the richness of the OSM schema, but finding the most appropriate mapping
can be quite time consuming.  For example, one can spend days translating an obscure
local language to determine the column headings and values in the context of OSM or
depending on their knowledge of Python/Javascript, create a custom translation value that
provides a mapping between the two schemas in a significantly shorter duration of time.

The following sections discuss high level issues associated with translating
files. For a more nuts and bolts discussion see the +convert+ section.

[[JavaScript-Translation]]
=== JavaScript Translation

Hootenanny support translation files written in both Python and JavaScript (AKA
ECMA Script). The JavaScript engine used by Hootenanny is the engine integrated
with Qt. See the http://qt-project.org/doc/qt-4.7/ecmascript.html[Qt ECMA Script
Documentation] for details on which operations are supported.

[[Overview]]
==== Overview

[[Special-Operations]]
===== Special Operations

In addition to the operations exposed by Qt, the user also has access to:

*  +require+ - Require a JavaScript module provided by Hootenanny. The list of
   supported modules is still being defined.
*  +print+ - Print a line to stdout
*  +debug+ , +logDebug+ - Print debug text to stdout using the Hootenanny
   logging facilities. Each message will include date/time, filename, and line
   number. E.g. logs if the +--debug+ flag has been set on the command line.
*  +logInfo+ - Print information text to stdout using the Hootenanny logging
   facilities. Each message will include date/time, filename, and line number.
*  +warn+ , +logWarn+ - Print warning text to stdout using the Hootenanny
   logging facilities. Each message will include date/time, filename, and line
   number.
*  +logError+ - Print error text to stdout using the Hootenanny logging
   facilities. Each message will include date/time, filename, and line number.
*  +logFatal+ - Print fatal text to stdout using the Hootenanny logging
   facilities. Each message will include date/time, filename, and line number.


[[Functions-Called-by-Hootenanny]]
===== Functions Called by Hootenanny

There are several functions that may be called by Hootenanny:

*  +initialize+ - An optional method that gets called before any other methods.
*  +finalize+ - An optional method that gets called after all other methods have
   been completed. This can be useful if you want to print out statistics on the
   translation.
*  +translateToOgr+ - Required by the +convert+ command to translate from OSM
   to a custom schema.
*  +translateToOsm+ - Required by the +convert+ command to translate from a
   custom schema to the OSM schema. For backwards compatibility reasons
   +translateAttributes+ is also supported, but +translateToOsm+ is preferred.
*  +getDbSchema+ - Required by the +convert+ command to get the custom schema
   that OSM data will be converted into.

[[Simple-Example]]
===== Simple Example


Below is about the simplest useful example that supports both +convert+. The following sections go into details on how these function are used.
------
// an optional initialize function that gets called once before any
// translateAttribute calls.
function initialize() {
    // The print method simply prints the string representation to stdout
    print("Initializing.")
}
// an optional finalize function that gets called once after all
// translateAttribute calls.
function finalize() {
    // the debug method prints to stdout when --debug has been specified on
    // the hoot command line. (DEBUG log level)
    debug("Finalizing.");
}
// A very simple function for translating NFDDv4's to OSM:
// - NAM column to OSM's name tag
// - TYP column to OSM's highway tag
// This is far from complete, but demonstrates the concepts.
function translateToOgr(tags, elementType, geometryType) {
    var attrs = {};
    if ('name' in tags) {
        attrs['NAM'] = tags['name'];
    }
    attrs['TYP'] = 0;
    if (tags['highway'] == 'road') {
        attrs['TYP'] = 1;
    }
    else if (tags['highway'] == 'motorway') {
        attrs['TYP'] = 41;
    }
    return { attrs: attrs, tableName: "LAP030" };
}
// A very simple function for translating from OSM's schema to NFDDv4:
// - name tag to NFDDv4's NAM column
// - highway tag to NFDDv4's TYP column
// This is far from complete, but demonstrates the concepts.
function translateToOsm(attrs, layerName) {
    tags = {};
    if (attrs['NAM'] != '') {
        tags['name'] = attrs['NAM']
    }
    if (attrs['TYP'] == 41) {
        tags['highway'] = 'motorway';
    }
    else {
        tags['highway'] = 'road';
    }
    return tags
}
// This returns a schema for a subset of the NFDDv4 LAP030 (road) columns.
function getDbSchema()
{
    var schema = [
        lap030 = {
            name: 'LAP030',
            geom: 'Line',
            columns: [
                {
                    name:'NAM',
                    type:'String'
                },
                { name:"TYP",
                  desc:"Thoroughfare Type" ,
                  optional:"O" ,
                  type:"enumeration",
                  enumerations:[
                     { name:"Unknown", value:"0" },
                     { name:"Road", value:"1" },
                     { name:"Motorway", value:"41" }
                  ] // End of Enumerations
                 } // End of TYP
            ]
        }
    ]
    return schema;
}
------


[[JavaScript-to-OSM-Translation]]
==== JavaScript to OSM Translation


The +translateToOsm+ method takes two parameters:

*  +attrs+ - A associative array of attributes and values from the source record.
*  +layerName+ - The name of the layer being processed. In the case of a Database source it will be the table name. In the case of a file input it will be the full path to the file. Frequently the +layerName+ is useful in decoding the type of feature being processed.

_Note_: The +translateToOsm+ was previously called +translateAttributes+ . Either name will still work, but +translateToOsm+ is preferred. If both are specified then +translateToOsm+ will be used.

This method will be called after the +initialize+ method is called when translating from an OGR format to a OSM schema. For instance if you call:

------
hoot convert -D schema.translation.script=tmp/SimpleExample.js "myinput1.shp myinput2.shp" myoutput.osm
------

The functions will be called in the following order:

.  +initialize+

.  +translateToOsm+ - This will be called once for every feature in myinput1.shp

.  +translateToOsm+ - This will be called once for every feature in myinput2.shp

.  +finalize+


[[Table-Based-Translation]]
===== Table Based Translation

For more advanced translations it may make sense to define a simple set of tables and use those tables to translate values. An example is below:

------
// create a table of nfdd biased rules.
var nfddBiased = [
    { condition:"attrs['SBB'] == '995'", consequence:"tags['bridge'] = 'yes'" }
];
// build a one to one translation table.
var one2one = [
    ['ROC', '1',    'surface',  'ground'],
    ['ROC', '2',    'surface',  'unimproved'],
    ['WTC', '1',    'all_weather', 'yes'],
    ['WTC', '2',    'all_weather', 'fair']
];
// build a more efficient lookup
var lookup = {}
for (var r in one2one) {
    var row = one2one[r];
    if (!(row[0] in lookup)) {
        lookup[row[0]] = {}
    }
    lookup[row[0]][row[1]] = [row[2], row[3]];
}
// A translateAttributes method that is very similar to the python translate
// attributes
function translateToOsm(attrs, layerName) {
    var tags = {};
    for (var col in attrs) {
        var value = attrs[col];
        if (col in lookup) {
            if (value in lookup[col]) {
                row = lookup[col][value];
                tags[row[0]] = row[1];
            }
            else {
                throw "Lookup value not found for column. (" + col + "=" + value + ")";
            }
        }
        else {
            for (var bi in nfddBiased) {
                print(attrs['SBB']);
                print(nfddBiased[bi].condition);
                print(eval(nfddBiased[bi].condition));
                print(nfddBiased[bi].consequence);
                if (eval(nfddBiased[bi].condition)) {
                    print("Condition true.");
                    eval(nfddBiased[bi].consequence);
                }
            }
        }
    }
    return tags;
}
------

[[OGR-to-OSM-Translation]]
==== OGR to OSM Translation

===== Translation File

The purpose of the translation file is to convert your custom Shapefile into the
OSM schema (http://wiki.openstreetmap.org/wiki/Map_Features). The translation
file is a Python script with a global function with the following definition:

* +def translateAttributes(attrs, layerName):+
** +attrs+ - A dictionary of attributes for a single feature to be translated.
** +layerName+ - The name of the layer being translated. This is provided in
   case multiple files are being translated at one time such as roads, bridges
   and tunnels. Sometimes this provides additional context when translated a
   feature.

The function must return either a dictionary of OSM tags or None if the feature
should be filtered from the data set. When +convert+ is launched Hootenanny
loads the specified Python file. The files in the +translations+ directory will
be included in the Python path. The same Python instance will be used for the
translations of all files in the _input_ list. This means that the script will
only be intialized once and then +translateAttributes+ will be called once for
each feature in all of the input files.

===== Example Translation Work Flow

Imagine you have a Shapefile named _MyRoads.shp_ for input with the following
attributes:

|==============================
| _STNAME_ | _STTYPE_ | _FLOW_
| Foo St.  | main     | 1
| Bar Rd.  | res      | 2
| Foo St.  | main     | 1
|==============================

In my notional example there are three columns with the following definitions:

* +STNAME+ - The name of the street.
* +STTYPE+ - The type of the street.
* +DIR+ - The flow of traffic, either 1 for one way traffic, or 2 for
  bidirectional traffic.

Hootenanny will call the translateAttributes method 3 times for this input. Each
call will contain the attributes for a given row. In this case the parameters
passed will be:

|===================================================================
| _attrs_                                           | _layerName_
| {"STNAME":"Foo St.", "STTYPE":"main", "FLOW","1"} | "MyRoads.shp"
| {"STNAME":"Bar Rd.", "STTYPE":"res", "FLOW","2"}  | "MyRoads.shp"
| {"STNAME":"Foo St.", "STTYPE":"main", "FLOW","1"} | "MyRoads.shp"
|===================================================================

The syntax above for _attrs_ is the dictionary syntax in Python. For more
details see the Python documentation
(http://docs.python.org/2/library/stdtypes.html#mapping-types-dict). You may
also have noticed that _layerName_ does not change during any of the calls. In
this case since we're only passing one input file the value will stay the same,
if we passed multiple files as input then the _layerName_ would change to
reflect the current input.

We must now write a translation file that will convert our input attributes into
a set of appropriate OSM tags. Using the
http://wiki.openstreetmap.org/wiki/Map_Features[Map Feature] reference on the
OSM wiki you can determine what is appropriate for a given input, but in this
notional example I'll give you the translations below:

* +STNAME+ - Equivalent to the OSM +name+ tag.
* +STTYPE+ - +main+ is equivalent to +highway=primary+ and +res+ is equivalent
  to +highway=residential+
* +DIR+ - 1 is equivalent to +oneway=yes+, 2 is equivalent to +oneway=no+.

So the input/output mapping we want is below:

*Inputs/Outputs Table*

|===============================================================================================================================
| _attrs_                                           | _layerName_   | _result_
| {"STNAME":"Foo St.", "STTYPE":"main", "FLOW","1"} | "MyRoads.shp" | {"name":"Foo St.", "highway":"primary", "oneway":"yes"}
| {"STNAME":"Bar Rd.", "STTYPE":"res", "FLOW","2"}  | "MyRoads.shp" | {"name":"Bar Rd.", "highway":"residential", "oneway":"no"}
| {"STNAME":"Foo St.", "STTYPE":"main", "FLOW","1"} | "MyRoads.shp" | {"name":"Foo St.", "highway":"primary", "oneway":"yes"}
|===============================================================================================================================

To accomplish this we can use the following translation script:

[source,python]
----
#!/bin/python
def translateAttributes(attrs, layerName):
    # Intialize our results object
    tags = {}
    # Is the STNAME attribute properly populated?
    if 'STNAME' in attrs and attrs['STNAME'] != '':
        tags['name'] = attrs['STNAME']
    # Is the STTYPE attribute properly populated?
    if 'STTYPE' in attrs and attrs['STTYPE'] != '':
        if attrs['STTYPE'] == 'main':
            tags['highway'] = 'primary'
        if attrs['STTYPE'] == 'res':
            tags['highway'] = 'residential'
    # Is the FLOW attribute properly populated?
    if 'FLOW' in attrs and attrs['FLOW'] != '':
        if attrs['FLOW'] == '1':
            tags['oneway'] = 'yes'
        if attrs['FLOW'] == '2':
            tags['oneway'] = 'no'
    # Useful when debugging. You can see print statements on stdout when Hootenanny is running
    #print "Input: " + str(attrs)
    #print "Output: " + str(tags)
    # Return our translated tags
    return tags
----

The translation script can also be written in JavaScript.

JavaScript notes:

* "tags.highway" is the same as "tags['highway']"
* OSM tags like "addr:street" MUST be specified using "tags['addr:street']" or
  you will get errors.

----
function translateToOsm(attrs, layerName) {
    tags = {};
    // Names
    if (attrs.STNAME) tags.name = attrs.STNAME;
    // Highways
    if (attrs.STTYPE == 'main') tags.highway = 'primary';
    if (attrs.STTYPE == 'res') tags.highway = 'residential';
    // Flow direction
    if (attrs.FLOW == '1') tags.oneway = 'yes';
    if (attrs.FLOW == '2') tags.oneway = 'no';
    // Print the input attrs for debugging:
    // This will print:
    // Input:STNAME: :Foo St.:
    // Input:STTYPE: :main:
    // etc
    // for (var i in attrs) print('Input:' + i + ': :' + attrs[i] + ':');
    // Print the output tags for debugging. The format is the same as for the
    // attrs
    // for (var i in tags) print('Output:' + i + ': :' + tags[i] + ':');
    return tags;
}
----

The translation scripts above will give the values found in the _Inputs/Outputs
Table_.

===== Example Python Translation File

The following script provides a more thorough example for translating
http://www.census.gov/geo/www/tiger/tgrshp2012/tgrshp2012.html[2010 Tiger road data]:

[source,python]
----
#!/bin/python
def translateAttributes(attrs, layerName):
    if not attrs: return
    tags = {}
    if 'FULLNAME' in attrs:
        name = attrs['FULLNAME']
        if name != 'NULL' and name != '':
            tags['name'] = name
    if 'MTFCC' in attrs:
        mtfcc = attrs['MTFCC']
        if mtfcc == 'S1100':
            tags['highway'] = 'primary'
        if mtfcc == 'S1200':
            tags['highway'] = 'secondary'
        if mtfcc == 'S1400':
            tags['highway'] = 'unclassified'
        if mtfcc == 'S1500':
            tags['highway'] = 'track'
            tags['surface'] = 'unpaved'
        if mtfcc == 'S1630':
            tags['highway'] = 'road'
        if mtfcc == 'S1640':
            tags['highway'] = 'service'
        if mtfcc == 'S1710':
            tags['highway'] = 'path'
            tags['foot'] = 'designated'
        if mtfcc == 'S1720':
            tags['highway'] = 'steps'
        if mtfcc == 'S1730':
            tags['highway'] = 'service'
        if mtfcc == 'S1750':
            tags['highway'] = 'road'
        if mtfcc == 'S1780':
            tags['highway'] = 'service'
            tags['service'] = 'parking_aisle'
        if mtfcc == 'S1820':
            tags['highway'] = 'path'
            tags['bicycle'] = 'designated'
        if mtfcc == 'S1830':
            tags['highway'] = 'path'
            tags['horse'] = 'designated'
    return tags
----

===== Example JavaScript Translation File

----
function translateToOsm(attrs, layerName) {
    tags = {};
    // Names
    if (attrs.FULLNAME && attrs.FULLNAME !== 'NULL') tags.name = attrs.FULLNAME;
    // Highways
    if (attrs.MTFCC == 'S1100') tags.highway = 'primary';
    if (attrs.MTFCC == 'S1200') tags.highway = 'secondary';
    if (attrs.MTFCC == 'S1400') tags.highway = 'unclassified';
    if (attrs.MTFCC == 'S1500') {
        tags.highway = 'track';
        tags.surface = 'unpaved';
    }
    if (attrs.MTFCC == 'S1600') tags.highway = 'road';
    if (attrs.MTFCC == 'S1640') tags.highway = 'service';
    if (attrs.MTFCC == 'S1710') {
        tags.highway = 'path';
        tags.foot = 'designated';
    }
    if (attrs.MTFCC == 'S1720') tags.highway = 'steps';
    if (attrs.MTFCC == 'S1730') tags.highway = 'service';
    if (attrs.MTFCC == 'S1750') tags.highway = 'road';
    if (attrs.MTFCC == 'S1780') {
        tags.highway = 'service';
        tags.service = 'parking_aisle';
    }
    if (attrs.MTFCC == 'S1820') {
        tags.highway = 'path';
        tags.bicycle = 'designated';
    }
    if (attrs.MTFCC == 'S1830') {
        tags.highway = 'path';
        tags.horse = 'designated';
    }
    return tags;
}
----

[[OSM-to-OGR-Translation]]
==== OSM to OGR Translation


Using JavaScript translation files it is now possible to convert from OSM to more typical tabular geospatial formats such as Shapefile or FileGDB. In order to convert to these formats some information will likely be lost and these translation files define which attributes will be carried across and how they'll be put into tables/layers.

The necessary functionality is accessed via two methods, +getDbSchema+ and +translateToOsm+ . Both methods are required.

The +getDbSchema+ method takes no arguments and returns a complex schema data structure that is described in theDB Schemasection.

The +translateToOsm+ method takes three arguments and returns an associative array values.
Arguments:

*  +tags+ - A associative array of tag key/value pairs from the source element/feature.
*  +elementType+ - The OSM element type being passed in. This is one of "node", "way", or "relation". See the OSM data model for more information.
*  +geometryType+ - The geometry type of the element being passed in. This is one of "Point", "Line", "Area" or "Collection". The value is determined based on both the element type and the tags on a given feature.

Returns:

*  +undefined+ if the feature should be dropped, or a single associative array with the following keys:
*  +attrs+ - An associative array of attributes where the key is the column name and the value is the cell's value. The cell's value does not need to be in the same data type as specified by the schema, but must be convertible to that data type. For instance returns a string zero ( +"0"+ ) and integer zero ( +0+ ) are both acceptable for an integer field. The attrs must be consistent with the table schema defined for the given +tableName+ .
*  +tableName+ - A string value the determines the table/layer that the feature will be inserted into. This must be one of the tables defined in the DB schema.


The methods will be called after the +initialize+ method is called when translating from an OGR format to a OSM schema. For instance if you call:

------
hoot convert -D schema.translation.script=tmp/SimpleExample.js myinput.osm myoutput.shp
------

The functions will be called in the following order:

.  +initialize+

.  +getDbSchema+

.  +translateToOgr+ - This will be called once for every element in myinput.osm that has at least one non-metadata tag. The metadata tags are defined in +$HOOT_HOME/conf/MetadataSchema.json+

.  +finalize+

This is most commonly accessed through the +convert+ command.


[[DB-Schema]]
===== DB Schema


Hootenanny supports converting OSM data into multiple layers where each layer has its own output schema including data types and column names.

The DB schema result is structured as follows:

------
// The top level schema is always defined as an array of table schemas
schema = [
  // each table is an associative array of key/values
  {
    // required name of the layer. This is the layer name that will be created.
    name: "ROAD_TABLE",
    // required geometry type for a table. Options are Point, Line and Polygon
    geom: "Line",
    // required array of columns in the table.
    columns: [
      {
        // required name of the column
        name: "NAM",
        // required type of the column.
        // Options are listed in "Supported output data types" below.
        type: "string",
        // Optional defValue field. If the column isn't populated in attrs then
        // this defValue will be used. If it isn't specified then the column
        // must always be specified in attrs.
        defValue: '',
        // Optional length field. If the column isn't populated then the default
        // field size is used as defined by OGR. If it is populated then the
        // value will be used as the field width.
        length: 255
      },
      // another column
      { name: "TYP", type: "enumeration",
        // enumerated values
        enumerations: [
          { value: 0 },
          { value: 1 }
        ]
      }
    ]
  }
  // any number of tables can be defined here.
];
------

Supported output data types:

*  +string+ - A variable length string.
*  +enumeration+ - A 32bit signed integer with specific acceptable enumerated values.
*  +double+ or +real+ - 64bit float
*  +integer+ or +long integer+ - Aliased to +enumeration+, but it doesn't require an +enumerations+ array.

The numeric data types support +minimum+ and +maximum+ . By default +minimum+ and +maximum+ are disabled. If min/max values are specified or an enumeration table is populated then Hootenanny will validate all output data before it is written. The following rules are used to determine if a value is valid:

* If the enumeration table is present ( +enumeration+ type only) then a value is valid. If the value is in the enumeration table then min/max bounds are ignored.
* If +maximum+ is specified then the value is invalid if it is greater than maximum.
* If +minimum+ is specified then the value is invalid if it is less than minimum.


[[File-Formats]]
==== File Formats

For the translation operations (and several others) Hootenanny utilizes the well known GDAL/OGR libraries. These libraries support a number of file formats including Shapefile, FileGDB, GeoJSON, PostGIS, etc. While not every format has been tested, many will work with Hootenanny without any modification. Others, such as FileGDB, may require a specially compiled version of GDAL. Please see the GDAL documentation and talk to your administrator for details.

Below are a discussion of some special handling situations when reading and writing to specific formats.


[[Shapefile]]
===== Shapefile

When writing shapefiles a new directory will be created with the basename of the specified path and the new layers will be created within that directory. For example:

------
hoot convert -D schema.translation.script=translations/MyTranslation.js input.osm output.shp
------

The above command will create a new directory called +output+ and the layers specified in the +translations/MyTranslation.js+ schema will be created as +output/<your layer name>.shp+ .


[[CSV]]
===== CSV


CSV files are created using the OGR CSV driver and will contain an associated +.csvt+ file that contains the column types. If you're exporting points then you will get an X/Y column prepended onto your data. If you're exporting any other geometry type then you will get a WKT column prepended that contains the Well Known Text representation of your data. If you would like to read from a CSV you must first create a VRT file as described in the OGR CSV documentation. E.g.

Creating a new CSV file:

------
hoot convert test-files/conflate/unified/AllDataTypesA.osm foo.csv translations/Poi.js
------

This uses a simple translation script ( +Poi.js+ ) that exports POI data and its associated tags. If you would then like to read that data create a new +.vrt+ file named +foo.vrt+ that contains the following:

------
<OGRVRTDataSource>
    <OGRVRTLayer name="foo">
        <SrcDataSource>foo.csv</SrcDataSource>
        <GeometryType>wkbPoint</GeometryType>
        <LayerSRS>WGS84</LayerSRS>
        <GeometryField encoding="PointFromColumns" x="X" y="Y"/>
    </OGRVRTLayer>
</OGRVRTDataSource>
------

Then to convert the file back into a .osm file run:

------
hoot convert -D schema.translation.script=translations/Poi.js foo.vrt ConvertedBack.osm
------


[[Buildings-Translation]]
=== Buildings Translation


In the simplest case a building is a way tagged with +building=yes+ . However, when it comes to 3D features buildings can get dramatically more complex. For a thorough discussion of Buildings and how they're mapped see the link:$$http://wiki.openstreetmap.org/wiki/Simple_3D_Buildings$$[OSM wiki page on Simple 3D Buildings] .


[[Translating-Building-Parts]]
==== Translating Building Parts


Some Shapefiles contain buildings that are mapped out as independent parts. Where each part refers to the roof type and height of a portion of the building. E.g. The Capital building might be mapped out as one large, low flat roof record and a second tall domed roof record. This provides for very rich data, but also a complex representation in OSM. Fortunately Hootenanny handles most of the heavy lifting for you.

To translate complex building parts simply translate them in the same way you would translate any other building. By default Hootenanny will then search through all the buildings and look for buildings that appear to be part of the same structure. If they're part of the same structure then a complex building will be created for you automatically. The complex buildings will take the form specified in the link:$$http://wiki.openstreetmap.org/wiki/Simple_3D_Buildings$$[Simple 3D Buildings] specification. The following section gives a specific example.


[[Complex-Building-Example]]
===== Complex Building Example

.Example of a Complex Building

image::images/image1348.png[]

In the above image there are three buildings; 123, 124, and 125. Building 123 is broken into two parts, a long rectangular section that is marked as a gabled roof and a squarish section that is marked with a flat roof. In a Shapefile that may look like the following:

|======
| name | roof_type
| 123 | gabled
| 123 | flat
| 124 | gabled
| 125 | gabled
|======

Using an abbreviated OSM JSON representation the resulting OSM data would be:

------
{ "type": "way", "id": 1, "tags": { "building": "yes", "addr:housenumber": "123", "building:roof:shape": "gabled" } }
{ "type": "way", "id": 2, "tags": { "building": "yes", "addr:housenumber": "123", "building:roof:shape": "flat" } }
{ "type": "way", "id": 3, "tags": { "building": "yes", "addr:housenumber": "124", "building:roof:shape": "gabled" } }
{ "type": "way", "id": 4, "tags": { "building": "yes", "addr:housenumber": "125", "building:roof:shape": "gabled" } }
------

Hootenanny will automatically detect that the two 123 buildings are part of the same building. This is done by asking the following questions:

* Do the two building share at least two consecutive nodes (share an edge) or does one building completely contain the other building?
* Do the non-part specific attributes of buildings match very closesly? (E.g. Are the addresses the same? Are the names the same? Ignore any differences in height or roof shape.)

If these two questions answer yes, then the building parts are grouped together. An arbitrary number of building parts may be grouped together in this way to create a larger building. Once the building parts are grouped some new elements are added to the map to represent the building parts as shown in the following OSM JSON snippet.

------
{ "type": "way", "id": 1, "tags": { "building:part": "yes", "building:roof:shape": "gabled" } }
{ "type": "way", "id": 2, "tags": { "building:part": "yes", "building:roof:shape": "flat" } }
{ "type": "way", "id": 3, "tags": { "building": "yes", "addr:housenumber": "124", "building:roof:shape": "gabled" } }
{ "type": "way", "id": 4, "tags": { "building": "yes", "addr:housenumber": "125", "building:roof:shape": "gabled" } }
{ "type": "way", "id": 5, "tags": { "building": "yes", "addr:housenumber": "125" } }
{ "type": "relation", "id": 1, "tags": { "type": "building", "building": "yes", "addr:housenumber": "123" },
    "members": [
        { "type": "way", "ref": 1, "role": "part" }
        { "type": "way", "ref": 2, "role": "part" }
        { "type": "way", "ref": 5, "role": "outline" } ] }
------

The astute reader may notice that a new way was created during this process. The new way, 5, is an outline of the entire building. This is done as part of the spec to be certain that older rendering engines don't ignore the complex building. Whenever building outlines are encountered by Hootenanny they are ignored and the more complex representation is used. However, Hootenanny will still generate building outlines. The building outline will always represent the union of all the building parts.


[[Disabling-Complex-Buildings]]
===== Disabling Complex Buildings


By default the when using the convert command to convert an OGR format to OSM +ogr2osm.simplify.complex.buildings+ is enabled.  If you
would like to disable the automatic construction of complex buildings from the individual parts then simply set
+ogr2osm.simplify.complex.buildings+ to false.  For example:

------
hoot convert -D schema.translation.script=MyTranslation -D ogr2osm.simplify.complex.buildings=false MyInput.shp MyOutput.osm
------

[[Common-Use-Cases]]
== Common Conflation Use Cases


The following sections describe some common use cases and how to approach them using Hootenanny.


[[Conflate-Two-Shapefiles]]
=== Conflate Two Shapefiles


The following subsections describe how to do the following steps:

. Prepare the input for translation

. Translate the Shapefiles into .osm files

. Conflate the Data

. Convert the conflated .osm data back to Shapefile

We'll be using files from the http://www.census.gov/geo/www/tiger/tgrshp2012/tgrshp2012.html[US Census Tiger] data and http://dcgis.dc.gov[DC GIS]

* Tiger Roads - link:$$ftp://ftp2.census.gov/geo/tiger/TIGER2012/ROADS/tl_2012_11001_roads.zip$$[ftp://ftp2.census.gov/geo/tiger/TIGER2012/ROADS/tl_2012_11001_roads.zip]
* DC GIS Roads - http://dcatlas.dcgis.dc.gov/catalog/download.asp?downloadID=88&downloadTYPE=ESRI[http://dcatlas.dcgis.dc.gov/catalog/download.asp?downloadID=88&downloadTYPE=ESRI]


[[Prepare-the-Shapefiles]]
==== Prepare the Shapefiles


First validate that your input shapefiles are both Line String (AKA Polyline) shapefiles. This is easily done with +ogrinfo+:

------
$ ogrinfo -so tl_2010_12009_roads.shp tl_2010_12009_roads
INFO: Open of `tl_2010_12009_roads.shp'
      using driver `ESRI Shapefile' successful.

Layer name: tl_2010_12009_roads
Geometry: Line String
Feature Count: 17131
Extent: (-80.967774, 27.822067) - (-80.448353, 28.791396)
Layer SRS WKT:
GEOGCS["GCS_North_American_1983",
    DATUM["North_American_Datum_1983",
        SPHEROID["GRS_1980",6378137,298.257222101]],
    PRIMEM["Greenwich",0],
    UNIT["Degree",0.017453292519943295]]
STATEFP: String (2.0)
COUNTYFP: String (3.0)
LINEARID: String (22.0)
FULLNAME: String (100.0)
RTTYP: String (1.0)
MTFCC: String (5.0)
------


[[Translate-the-Shapefiles]]
==== Translate the Shapefiles


Hootenanny provides a link:$$User_-_convert.html$$[convert] operation to translate and convert shapefiles into OSM files. If the projection is available for the Shapefile the input will be automatically reprojected to WGS84 during the process. If you do a good job of translating the input data into the OSM schema then Hootenanny will conflate the attributes on your features as well as the geometries. If you do not translate the data properly then you'll still get a result, but it may not be desirable.


[[Crummy-Translation]]
===== Crummy Translation


The following translation code will always work for roads, but drops all the attribution on the input file.

[source,python]
------
#!/bin/python
def translateAttributes(attrs, layerName):
    if not attrs: return
    return {'highway':'road'}
------


[[Better-Translation]]
===== Better Translation


The following translation will work well with the tiger data.

[source,python]
------
#!/bin/python
def translateAttributes(attrs, layerName):
    if not attrs: return
    tags = {}
    # 95% CE in meters
    tags['accuracy'] = '10'
    if 'FULLNAME' in attrs:
        name = attrs['FULLNAME']
        if name != 'NULL' and name != '':
            tags['name'] = name
    if 'MTFCC' in attrs:
        mtfcc = attrs['MTFCC']
        if mtfcc == 'S1100':
            tags['highway'] = 'primary'
        if mtfcc == 'S1200':
            tags['highway'] = 'secondary'
        if mtfcc == 'S1400':
            tags['highway'] = 'unclassified'
        if mtfcc == 'S1500':
            tags['highway'] = 'track'
            tags['surface'] = 'unpaved'
        if mtfcc == 'S1630':
            tags['highway'] = 'road'
        if mtfcc == 'S1640':
            tags['highway'] = 'service'
        if mtfcc == 'S1710':
            tags['highway'] = 'path'
            tags['foot'] = 'designated'
        if mtfcc == 'S1720':
            tags['highway'] = 'steps'
        if mtfcc == 'S1730':
            tags['highway'] = 'service'
        if mtfcc == 'S1750':
            tags['highway'] = 'road'
        if mtfcc == 'S1780':
            tags['highway'] = 'service'
            tags['service'] = 'parking_aisle'
        if mtfcc == 'S1820':
            tags['highway'] = 'path'
            tags['bicycle'] = 'designated'
        if mtfcc == 'S1830':
            tags['highway'] = 'path'
            tags['horse'] = 'designated'
    return tags
------

To run the tiger translation put the above code in a file named +translations/TigerRoads.py+ and run the following:

------
hoot convert -D schema.translation.script=TigerRoads tmp/dc-roads/tl_2012_11001_roads.shp tmp/dc-roads/tiger.osm
------

The following translation will work OK with the DC data.

[source,python]
------
#!/bin/python
def translateAttributes(attrs, layerName):
    if not attrs: return
    tags = {}
    # 95% CE in meters
    tags['accuracy'] = '15'
    name = ''
    if 'REGISTERED' in attrs:
        name = attrs['REGISTERED']
    if 'STREETTYPE' in attrs:
        name += attrs['STREETTYPE']
    if name != '':
        tags['name'] = name
    if 'SEGMENTTYP' in attrs:
        t = attrs['SEGMENTTYP']
        if t == '1' or t == '3':
            tags['highway'] = 'motorway'
        else:
            tags['highway'] = 'road'
    # There is also a one way attribute in the data, but given the difficulty
    # in determining which way it is often left out of the mapping.
    return tags
------

To run the DC GIS translation put the above code in a file named +translations/DcRoads.py+ and run the following:

------
hoot convert -D schema.translation.script=DcRoads tmp/dc-roads/Streets4326.shp tmp/dc-roads/dcgis.osm
------


[[Conflate-the-Data]]
==== Conflate the Data


If you're just doing this for fun, then you probably want to crop your data down to something that runs quickly before conflating.

------
hoot crop tmp/dc-roads/dcgis.osm tmp/dc-roads/dcgis-cropped.osm "-77.0551,38.8845,-77.0281,38.9031"
hoot crop tmp/dc-roads/tiger.osm tmp/dc-roads/tiger-cropped.osm "-77.0551,38.8845,-77.0281,38.9031"
------

All the hard work is done. Now we let the computer do the work. If you're using the whole DC data set, go get a cup of coffee.

------
hoot conflate tmp/dc-roads/dcgis-cropped.osm tmp/dc-roads/tiger-cropped.osm tmp/dc-roads/output.osm
------


[[Convert-Back-to-Shapefile]]
==== Convert Back to Shapefile


Now we can convert the final result back into a Shapefile.

------
hoot convert -D shape.file.writer.cols="name;highway;surface;foot;horse;bicycle" tmp/dc-roads/output.osm tmp/dc-roads/output.shp
------


[[Snap-GPS-Tracks-to-Roads]]
=== Snap GPS Tracks to Roads


. Create a translation file for "translating" your GPS tracks. This typically just adds the accuracy field. E.g. +accuracy=5+

. Convert your GPX file into an OSM file where each track is now a way.
+
------
hoot convert -D schema.translation.script=GpsTrack "$HOME/MyTracks.gpx;tracks" tmp/MyTracks.osm
------
. Use the special track snapping conflation manipulation to snap your tracks to an existing road network and convert to Shapefile.
+
------
hoot conflate -D conflator.manipulators=hoot::WaySnapMerger HighQualityRoads.osm tmp/MyTracks.osm tmp/MySnappedTracks.osm
hoot convert -D shape.file.writer.cols "hoot:max:movement;hoot:mean:movement;hoot:score;name;foot" tmp/MySnappedTracks.shp tmp/MySnappedTracks.osm
------



[[Maintaining-per-node-attributes]]
==== Maintaining per node attributes


If you have node attributes that you want to keep you can use the +hoot::PointsToTracksOp+ operation to join the nodes after translation. This requires two fields on each node:

*  +hoot:track:id+ - The id of the track that the node belongs to. The id is simply treated as a string. Nodes with like ids will be grouped together.
*  +hoot:track:seq+ - The sequence of the nodes within the track (way). This is treated as a string and sorted as a string where the smallest value is at the beginning of the track. Be certain to avoid problems with integers during translation. E.g. "13", "112" will not sort properly, but "013", "112" will sort properly. It is also recommended to use +hoot::MergeNearbyNodes+ as a poor man's line simplification to speed the process up a bit. If this causes problems with your data you can safely drop it.

The command used with a GPX input file is:

------
hoot convert -D "convert.ops+=hoot::PointsToTracksOp" -D schema.translation.script=GpsTrack "$HOME/MyTracks.gpx;track_points" tmp/MyTracks.osm
------

An example translation file is:

[source,python]
------
#!/bin/python
def translateAttributes(attrs, layerName):
    if not attrs: return
    tags = attrs
    tags['accuracy'] = '5'
    tags['highway'] = 'road'
    if 'track_fid' in attrs:
        tags['hoot:track:id'] = attrs['track_fid']
        tags['hoot:track:seq'] = "%09d" % int(attrs['track_seg_point_id'])
    return tags
------

*Special Rule* If all the nodes in a track have the same +highway=*+ setting then the highway attribute will be moved from the node to the way.


[[Add-NSG-TLM-Symbology-to-a-FileGeodatabase]]
=== Add NSG TLM Symbology to a FileGeodatabase

==== Overview
ESRI ArcMap can use Visual Representation rules to display symbology. Hootenanny is able to export Topographic Data Store (TDS) compliant data in a FileGeodatabase that is able to have default symbology applied to it. The command line procedure to create default symbology is as follows.

References:

* http://resources.arcgis.com/en/help/main/10.2/index.html#/What_are_representations/00s50000004m000000/
* http://resources.arcgis.com/en/help/main/10.2/index.html#/What_are_visual_specifications/0103000001w9000000/

==== Requirements
The main requirement is access to a copy of ESRI ArcGIS with the following:

* ArcGIS Standard or ArcGIS Desktop license
* Production Mapping Extension & license
* Defense Mapping Extension & license

==== Process:

Get an empty TDS template FileGeodatabase::
* From the ArcGIS Defence Mapping Extension install location.
+
----
C:\Program Files\ArcGIS\EsriDefenseMapping\Desktop10.2\Tds\Local\Schema\Gdb\LTDS_4_0.zip
----
* Unpack this Zip file and copy the "LTDS_4_0.gdb" File GeoDatabase to your Hootenanny working directory.

Run Hootenanny and add your data to the template File GeoDatabase::
* Add the "Append Data" flag: +ogr.append.data+
* Add the template File GeoDatabase to write to.
+
----
hoot convert -D schema.translation.script=GpsTrack -D ogr.append.data="true" your_data.osm LTDS_4_0.gdb
----

Transfer the LTDS_4_0.gdb to the machine that has ESRI ArcGIS installed::
* Place it in a convenient location

Set the "Product Library" in ArcMap::
Reference: http://resources.arcgis.com/en/help/main/10.2/index.html#//0103000001p0000000
* Copy `C:\Program Files\ArcGIS\EsriDefenseMapping\Desktop10.2\Tds\Local\Product Library\LTDS_4_0_Product_Library.zip` to where you saved the Hoot File GeoDatabase
* Unzip `LTDS_4_0_Product_Library.zip` to get `LTDS_4_0_Product_Library.gdb`
* Open ArcMap:
** Click on `Customize->Production->Product Library`
** Right Click on `Product Library`
** Click on `Select Product Library`
** Navigate to wherever you saved the `LTDS_4_0_Product_Library.gdb` and select it.


Calculate the Visual Specifications::
Reference: http://resources.arcgis.com/en/help/main/10.2/index.html#/Calculate_Visual_Specifications/01090000001w000000/
* Open ArcCatalog
* Run `Toolboxes->System Toolboxes->Production Mapping Toolbox->Symbology->Calculate Visual Specifications`:
** *Input Features* Browse to where the Hootenanny File GeoDatabase is saved and select all of the features inside the `LTDS` feature dataset
** *Visual Specification Workspace* Browse to and select `C:\Program Files\ArcGIS\EsriDefenseMapping\Desktop10.2\Tds\Local\Cartography\Symbology\LTDS_4_0_NSG_Visual_Specification.mdb`
** Select `LTDS_NSG::50K` for 50K TLM symbology or `LTDS_100K::100K` for a 100K TLM symbology.
** Click on `OK`
** Wait for it to finish. It will take a while.

View the Default Symbology::
* Open ArcMap
* Add the +LTDS_4_0.gdb+ dataset

