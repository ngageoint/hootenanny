
== Introduction

Mapping data is a commodity with a demand that continues to increase with the
growth of geographically-enabled applications.  Mapping data is generated by
commercial, government, and more recently by crowd-sourced methods such as
<<OpenStreetMap,OpenStreetMap>> and WikiMapia footnote:[ WikiMapia or Wikimapia
is an open-content collaborative mapping project that aims to mark and
describe all geographical objects in the world. http://wikimapia.org ]. The
challenge is that no one source has it all, but collectively multiple resources
provide the best picture.   "Conflation of maps refers to a combining
of two digital map files to produce a third map file which is 'better' than each
of the component source maps." (<<saalfeld1987,Saalfeld 1985>>) A conflated
dataset is critical to making a better map and is also the foundation to more
effective geospatial analytics (i.e., geocoding, routing).

The task of selecting which dataset(s) to use and merge becomes a daunting process 
often requiring a case-by-case evaluation where analysts are forced to resort to 
multiple ad-hoc conflation techniques (automated and manual) which quickly becomes 
labor intensive and stove-piped. As such, multiple conflation efforts occur in 
isolated projects by numerous users for specific purposes with varying workflows, 
rules, schemas, formats, and data quality. This type of approach creates inefficiencies 
and lacks standardization which is required for large scale organizational use, 
maintenance, and development activities.

To this end, Hootenanny was developed to provide an open source standards based 
approach to geospatial vector-data conflation. Hootenanny is designed to facilitate 
automated and semi-automated conflation of critical Foundation GEOINT features in 
the topographic domain, namely roads (polylines), buildings (polygons), and 
points-of-interest (POIs) (points). Conflation happens at the dataset level, 
where the user’s workflow determines the best reference dataset and source content, 
geometry and attributes, to transfer to the output map. The input data must be 
normalized to allow processing and matching of features and attributes from different 
schemas. Hootenanny internal processing leverages the key value pair structure of 
OpenStreetMap (OSM) for improved utility and applicability to broader user groups, 
e.g. normalized attributes can be used to aid in feature matching and OSM’s free 
tagging system allows the map to include an unlimited number of attributes describing 
each feature (OpenStreetMap/Map Features, 2015).

Historically, conflation applications have been specialized desktop tools which 
required a niche expertise with reoccurring licensing requirements and difficult 
to customize or add functionality. However, open source software provides an 
attractive business model where the user community can contribute, customize, 
share, and help maintain the software in an interactive environment. For this 
reason, Hootenanny is being developed under the open source General Public License 
(GPL) and will be hosted on the National Geospatial-Intelligence Agency’s (NGA) 
link:$$https://github.com/ngageoint/hootenanny$$[GitHub website] (NGA GitHub, 2015).

=== History

[quote, Saalfeld, 1987]
____________________________________________________________________
_"Having more than one set of maps to conflate is only now being experienced by
users outside of the Bureau of the Census. Digital files were very expensive to
create and, as a result, were very expensive to buy; and users generally did not
have, nor did they seek, access to more than one digital file of the same
region."_
____________________________________________________________________

This quote comes from a report made when the Census Bureau was preparing for the
1990 Census. At that time, the Census Bureau was researching and developing
methods to merge data from the United Stated Geological Survey into the GBF/DIME
to create what eventually became the TIGER database for the 1990 Census. The
goal of the integration effort was to "consolidate their diverse source
materials into a single coherent map" (<<saalfeld1987,Saalfeld 1987>>).

=== The Current Picture

More than 25 years later, automatically conflated data sets are still not common
place, with analysts routinely required to apply one-off manual techniques to
support a project or simply to use display techniques to make two overlapping
datasets appear as one.  In the first case the manually conflated data sets take
an inordinate amount of time to produce, and the techniques are very difficult
to scale into larger areas.  In the latter case, display techniques only benefit
cartographic rendering and provide little to no value to analytic capabilities
because the two datasets are not integrated into one.

=== Scope/Rationale of Effort

The purpose of this research is developing a fully automated conflation
capability that scales to support big geographic datasets. The rationale is that
such a conflation capability could create a consolidated data set that leverages 
feature content from diverse inputs without involving expensive manual conflation. 
While the quality of the output will not be the same as manually conflated data, 
the cost should be much lower and the quality good enough for many uses.

=== Use Cases

The following use cases highlight where automated conflation can improve
efficiency and reduce cost.

==== Mission specific datasets (MSDS) stored locally on analyst workstations

These data are a result of analysts compiling, storing, and using geospatial
feature data stored locally. These data sets are usually considered "one-off,"
since their usage is typically limited to a single use or user.  While these
data may not be collected using an extraction specification such as FACC
footnote:[Feature and Attribute Coding Catalogue] or NFDD footnote:[Feature
Data Dictionary], they nonetheless could contain additional geometry and/or
attribution that could enhance a global dataset.  Conflation tools could be used
to streamline the integration process and reduce the cost for analysts to upload
their MSDS into a "global view."

==== Spotlight updates

These updates represent the integration of an enhanced dataset that is generally
produced on a larger mapping scale.  Merging Urban Feature Data (UFD) with
Foundation Feature Data (FFD) would be an example of this, where the conflation
process would result in integrating high-resolution "urban scale" interurban
features into moderate scale geospatial data. 

==== Multi-source integration

Sometimes, analysts have many datasets of similar scale to choose from. When
this is the case, the individual datasets may introduce unique features that
collectively provide more complete attribution and geometry of features. A fully
automated tool to accomplish this could provide another option when deciding how
best to weigh the low cost of automated conflation versus the higher quality
that is sometimes yielded by the manual process.

==== Integrate database change sets

Datasets such as OpenStreetMap (OSM) are constantly being edited, so maintaining
internally modified copies is impractical. However, using a fully automated
conflation tool to conflate internal NGA data with OSM could be performed
periodically over global data sets. This approach would begin to enable a
recurrent temporal correction to NGA data.

