
== Generic Line Conflation

Hootenanny supports "highway" conflation as defined by OpenStreetMap (OSM). In
OSM jargon a "highway" refers to paths, roads, cart tracks and possibly bridges
and tunnels. All these linear features are supported during conflation using
an existing statistical model tuned for highways.

However, in some cases it isn't desirable to use a predefined model to match and
merge linear features. For instance you may have one of the following data types
without any appropriate training data for the model:

* Rivers & Streams
* Walls
* Shorelines

The following sections discuss an approach to conflating lines in the general
case. This approach is currently being investigated and will likely generate
more discussion and refinement.

=== Background

Hootenanny currently supports generic conflation of data sets through the
JavaScript (<<JS,JS>>) Application Programmers Interface (API). See the
_JavaScript Conflation Reference_ <<hootuser>> for details. Using this interface
pairwise matching and merging is supported, as well as the merging of complex
linear situations such as partial matches, one to many matches, and many to many
matches.

[[conflation-flow]]
.Conflation Flow
[graphviz]
---------------------------------------------------------------------
digraph G
{
  rankdir = LR;
  node [shape=record,width=2,height=1,style=filled,fillcolor="#e7e7f3"];
  split [label = "Split Data Based\non Subgraphs",shape=diamond,height=1.5];
  optimize [label = "Optimize Set\nof Matches"];
  graphBased [label = "Create Graph\nBased Merges"];
  apply [label = "Apply Merges"];
  "Ingest & Clean" ->
  "Estimate Pairwise\nRelationships" ->
  split ->
  optimize ->
  apply ->
  "Export";
  split -> graphBased -> apply;
}
---------------------------------------------------------------------

<<conflation-flow>> depicts the typical flow of data through a Hootenanny
conflation process. The steps are summarized below:

* Ingest & Clean - Read the files off of disk and perform any necessary cleaning
  operations (e.g. remove duplicate line strings).
* Estimate Pairwise Relationships - This process estimates whether or not two
  features represent the same entity. The exact approach is feature type
  dependent.
* Split Data Based on Subgraphs - If the conflation requires consistency (e.g.
  groups of buildings/POIs), then it goes into the graph based Matches,
  otherwise matches are sent to the optimize step.
* Optimize Set of Matches - Select the best set of consistent matches.
  Inconsistent or conflicting matches will be dropped.
* Create Graph Based Merges - Keep all subgraphs of matches. If they are
  inconsistent or conflicting then mark them all as needing review. Otherwise,
  merge them all together.
* Apply Merges - Given a set of matches, merge all the features into a single
  feature or set of features.
* Export - Export the data out to persistent storage.

The approach discussed below addresses customizations of three of the above
steps:

* Estimate Pairwise Relationships
* Split Data Based on Subgraphs
* Apply Merges

The following sections discuss these customizations in more detail.

See also:

* <<UnifyingConflation,Unifying Conflation>>
* <<RoadConflation,Road Conflation>>

==== Example Data Sets

[[coastlines]]
.Coastlines
image::algorithms/images/Coastlines.png[Coastlines at two scales,scalewidth="50%"]

In <<coastlines>> is an example of two coastlines extracted at two different
scales. While there are obvious similarities there are also significant
differences. Especially with regard to the angular differences. Using a standard
road conflation routine with coastlines would likely produce sub-optimal
results, but conflating with a user derived set of rules could perform quite
well. 

=== Approach

For the next phase of development we propose creating a new JavaScript API for
conflating linear data. This new approach will support some of the more complex
linear conflation situations such as one to many, many to many and partial
matches. In the following sections we will present an approach that provides the 
mechanisms for defining the following functionality:

* Designate line candidate bounds
* Designate an element as a candidate for conflation
* Estimate subline relationships
* Determine line conflicts
* Merge lines

This generic approach to conflation rules will allow the user to define a number
of different conflation algorithms without getting into the details of the
geometry manipulations.

==== Designate Line Candidate Bounds

Before any match relationships are established a pair of lines or sublines must
be designated as a potential match. This is generally comparable to the maximum
distance provided in point matching, but rather than matching based solely on
minimum distance we look at the distance between point pairs along the string.

Hootenanny supports a fast subline matching routine (maximal nearest subline)
and a more exhaustive maximal subline algorithm. They each have trade-offs with
regards to performance versus computational complexity. These algorithms also
take parameters to determine the maximum distance allowable when calculating
matching sublines as well as the maximum difference in angle when calculating
sublines.

Hootenanny exposes the following parameters for determining subline match
bounds:

* Maximum distance (Provided as a multiple of the Circular Error)
* Maximum angular difference
* Maximal subline routine

==== Designate an Element as a Candidate for Conflation

For optimization purposes we don't want to evaluate all element pairs for
conflation if we know in advance that we can filter based on very coarse
criteria. For example, if we're conflating rivers then there is no need to look
at roads, power-lines or walls. Failing to implement this method will not likely
impact the final result, but will dramatically reduce performance.

The exposed method is named +isCandidate+ and takes a single element as input.
If the element is a candidate for conflation the rest of the routines will be
evaluated. Otherwise, the element will be ignored for the purposes of this
specific line conflation routine.

Routines are exposed to JavaScript for determining if the element is a candidate
as needed. For example +isBuilding+, +isArea+, +isLinear+, etc.

See _Modifying Hootenanny with JavaScript_ <<hootuser>> for details.

==== Estimate Subline Relationships

Given a pair of line segments, assign a score to the relationship. This score is
in the form of 3 numbers that represent the probability of match, miss and
review (sums to one). For many expert systems this will simply return 1 for the
relationship that is most likely, but the closer values are to true
probabilities the better the optimization step will perform.

The relationship estimates are used by Hootenanny as described in the following
section to determine the best set of matches to merge or mark as needing review.

See _Modifying Hootenanny with JavaScript_ <<hootuser>> for the interface.

==== Merge Lines

Finally, the JavaScript routines must designate how the lines are to be merged.
This falls into two broad categories.

1. Optimize - The best (or near best) set of matches should be selected for
   merging.
2. Whole Group - The features being merged are treated as a whole group
   together. They should only be merged if the answer is unambiguous. If the
   answer is ambiguous or conflicting then flag the whole set of features as
   needing review.

The JavaScript code designates the conflation as either of the merge approaches
above. Using that information the Hootenanny core decides how to handle complex
match situations and creates a set of final matches that must be turned into
merge operations.

The JavaScript code defines how a merge operation is executed. At a minimum the
user must define how the following operations are performed:

1. Merge Tags - The JS code must either define a custom tag merging strategy or
   rely on the default Hootenanny tag merge operation.
2. Merge Geometries - The JS code must call one of the existing Hootenanny
   geometry merging techniques to merge the geometries.

A simple `snapWays` method is exposed to the user for snapping one way to
another. This is the most common linear use case. As the needs evolve more merge
methods will be exposed.

See also:

* `snapWays` in <<hootuser>>

=== Exposed Hootenanny APIs

As the effort evolves we will surely determine additional points to expose
appropriate functionality from Hootenanny code to the JS interface. At that
point some of the interfaces described below may become irrelevant and many more
will become necessary. Below are a number of examples to give an idea of the
interfaces that may be exposed.

* Calculate mean distance - Calculate the mean distance between two line strings.
* Calculate maximum angular difference - Calculate the maximum angular
  difference between two line strings.
* Snap line string - Snap one line string to another.
* Average line strings - Average two line strings together.

Example uses of these methods can be found in the hoot source code under
`rules/LineStringGeneric.js`.

=== Generic Line Conflation Test

The various test configuration are described in the sections below. All of these
tests compare a generic line conflation technique against the tuned Random
Forest Model. The generic technique is in the hoot distributable as
`rules/LineStringGeneric.js`.

NOTE: A compromise was struck on these tests between time spent and
thoroughness. These tests give a general impression of the performance
characteristics, but more time could be spent to provide better explanations and
more diverse scenarios. See <<GenericLineConflationFutureWork>> for details.

==== DC Tiger Perty

This test configuration uses Tiger data over the DC region to determine
performance. The `perty-test` command is used with the following configuration:

-----
{
  "match.creators" : "hoot::ScriptMatchCreator,LineStringGeneric.js",
  "merger.creators" : "hoot::ScriptMergerCreator",
  "uuid.helper.repeatable": "true",
  "perty.seed" : 0,
  "perty.test.num.runs": 1,
  "perty.test.num.simulations": 5
}
-----

During each test run the `perty.systematic.error.x` and 
`perty.systematic.error.y` values are modified to vary the amount of error in
the tests.

==== Jakarta Easy Test

This test scenario uses two manually conflated data in a simple region of
Jakarta as a baseline for evaluation. The methods described in the 
<<Evaluation,Evaluation>> section are used for comparison. A higher value means
closer agreement with the manually conflated data.

==== Jakarta Spaghetti Test

Similar to above, this test scenario uses two manually conflated data sets, but
in a more complex interchange region of Jakarta. The methods described in the
<<Evaluation,Evaluation>> section are used for comparison. A higher value means
closer agreement with the manually conflated data. Maintaining proper network
topology is much more complicated in this scenario due to multiple overpasses,
one way streets and tunnels.

==== Manually Matched

This data set contains test data over the regions described in _Classify the
Match_ section. This is likely the most comprehensive of the tests for match
results as it uses data over several regions extracted using several different
methods. However, this test does very little for evaluating how well features
are merged.

==== Test Results

[[GenericConflationQuality]]
.Conflation Quality
image::algorithms/images/GenericLineTestGraph.png[Random Forest vs. Generic Line Road Conflation Performance,scalewidth="50%"]
////
#TODO: replace with MPL - #267
#[gnuplot, algorithms/GenericLineTestGraph.png]
#------
#set title "Random Forest vs. Generic Line Road Conflation Performance\nHigher is Better"
#set auto x
#set yrange [.5:1.1]
#set style data histogram
#set style histogram cluster errorbars gap 1
#set style fill solid 0.5 border -1
#set boxwidth 0.9
#set xtic rotate by -70 scale 0 font "arial,10"
#set ylabel "Score"
#set bmargin 7
#plot "algorithms/GenericLineTest.dat" using 2:3:4:xticlabels(1) title columnheader(2), \
#    '' using 5:6:7 fill solid 0.5 title columnheader(5)
#------
# start at an attempt with MPL to do the same thing as the above gnuplot code
#["mpl", "algorithms/GenericLineTestGraph.png"]
#---------------------------
#figure(figsize=(5,2.5))
#title('Random Forest vs. Generic Line Road Conflation Performance\nHigher is Better')
#ylabel('Score')
#ylim(0.5,1.1)
#data = genfromtxt('/data/hoot/docs/algorithms/GenericLineTest3.dat', delimiter=',', missing_values=0, names=True, dtype=None)
#xticks(arange(2), (data[0][0], data[1][0]), rotation=17)
#margins(20)
#subplots_adjust(bottom=0.15)
#tick_params(labelsize=10)
#--------------------------
////

In <<GenericConflationQuality>> 90% confidence interval error bars are presented
when relevant.  For some tests no confidence intervals are generated and have
been omitted (e.g. raster comparison and calculating the number of correct
matches). In the case of DC Perty 20m the error bars are omitted due to a
known limitation.

You can see that the scores are generally comparable. The more complex spaghetti
example has slightly higher scores. The manually matched data has dramatically
higher scores for the trained model versus the generic rules. The exact reason
for this requires more investigation.

[[Conflation Speed]]
.Conflation Speed
image::algorithms/images/GenericLineTestTimingGraph.png[Random Forest vs. Generic Line Road Conflation Timing,scalewidth="50%"]
////
#TODO: replace with MPL - #267
#[gnuplot, algorithms/GenericLineTestTimingGraph.png]
#------
#set title "Random Forest vs. Generic Line Road Conflation Timing\nLower is Better"
#set auto x
#set yrange [0:400]
#set style data histogram
#set style histogram cluster gap 1
#set style fill pattern 2 border -1
#set boxwidth 0.9
#set xtic rotate by -70 scale 0 font "arial,10"
#set ylabel "Time Elapsed in Seconds"
#set bmargin 7
# The sed silliness limits to only use the rows that contain timing data
#plot "<(sed -n '1,5p;9p;13,1000p' algorithms/GenericLineTest.dat)" \
#  using 8:xticlabels(1) title columnheader(8), \
#  '' using 9 fill pattern 2 title columnheader(9)
#------
////

The image above shows the conflation speed for some of the test runs discussed
above. The Perty test runs include the entire perty operation (perturbing,
conflating and scoring the data). Re-running those experiments without the
testing times included will likely reduce the overall runtime, but the times
will likely still be very similar as most of the compute time was spent
conflating. The Easy and Spaghetti tests show a much more dramatic difference in
time demonstrating that the runtime difference will be data set specific.
Further work is required if we want to quantify when to expect those
differences.

=== River Conflation

==== Overview

A variant of generic line conflation specific to conflating rivers (linear waterways) is available within 
Hootenanny.  A river conflation model was developed based on test scenarios using manually matched data
in the regions of Haiti and San Diego, USA as a baseline for evaluation.

==== Approach

The goal for the initial conflation of rivers was that the number of correctly conflated features plus 
the number of features marked for manual review would equal at least 80% of the overall conflated 
features in each test dataset.  It is very likely that over time a higher accuracy than 80% could be
achieved with Hootenanny, however, this seemed a realistic goal for the initial implementation of generic river 
conflation.  An attempt was made to have a as close to a minimum of 200 manually matched features as possible in each dataset, 
while keeping datasets small enough that a single test against them could be run in roughly ten minutes or 
less.  One dataset at a time was tested against until the conflation performance goal was reached before moving 
onto testing against additional datasets.  After all datasets were tested against, a final model 
was constructed that performed acceptably against all tested datasets.  

The original plan was to test against three datasets.  It was later found that the third acquired dataset, rivers in 
Mexico, contained longer rivers with higher node counts such that the subline matching routine took
unreasonable amounts of time to complete.  Optimizing the subline matching to address this issue requires work outside of
the scope of this initial river conflation task (see "Future Work" section), therefore, 
the third dataset was not tested against.

During testing, an optimal search radius (controlled by the "error:circular" attribute) was first determined empirically for each 
river dataset.  After testing, the capability to automatically calculate this value was added, so 
manually determining it is no longer necessary but is allowed in the case the auto-calculation does
not provide an acceptable value.

Initially, during testing reference rubbersheeting was then used to bring the OSM river data towards the dataset it was being
conflated with.  This helped increase the conflation accuracy.  Later during testing, the addition 
of the automatically calculated search radius used without rubber sheeting provided even better results. 
Using the automatically calculated search radius precludes rubber sheeting of the input data since 
tie points from the rubber sheeting algorithm are used to calculate the search radius.

Several features were extracted from the river data tested against to help determine which ones
could be used to most accurately conflate the data.  Weka (<<hall2009>>) was used to build models
based on extracted features.  After running many tests, the two most influential features were found 
to be weighted shape distance and a sampled angle histogram value.  Those features were used to 
derive a model for conflating the river data.  

Weighted shape distance is similar to Shape Distance as described in <<savary2005>>.  

Angle histogram extraction works by calculating the angle of each line segment in a line string 
and adding that angle to a histogram where the weight is the length of the line segment.  
A histogram is built up in this way for both input lines, then normalized, and the difference calculated.  
To conflate rivers, a sampled angle histogram value was derived, which allows for specifying a configurable 
distance along each line segment to sample the angle value.  The distance from the sampled location to 
calculate the directional heading along the way is also configurable.

Two additional approaches were attempted that did not increase performance of the river conflation model 
against the datasets tested, but are worth mentioning: increasing the unnecessary reviewable feature 
count to aid in decreasing the incorrectly conflated feature count and weighting matches between
extracted sublines closer in distance higher than those that were further apart.

See the User Guide section on river conflation for details on configuration options.

==== Results

.*Generic River Conflation Test Results*
[width="100%"]
|======
| *AOI* | *Manually Matched Feature Count* | *Percentage Correctly Conflated* | *Percentage Marked for Unecessary Review* | *Percentage Combined Correct and Reviewable* | *Time Elapsed (HH:MM:SS)*
| Haiti  | 490 | 85.5 | 0.4 | 85.9 | 00:12:07
| San Diego | 784 | 55.3 | 17.0 | 72.3 | 02:19:22
|======

The goal of 80% combined correct and reviewable was met with the Haiti river data, but was not met with the San Diego river data.  
Future work listed in a following section should help to increase the conflation accuracy further.

==== Future Work

There are opportunities for improving river conflation using Hootenanny:

1. Adding a heuristic to subline matching that effectively handles river data
with larger amounts of nodes and overlapping sublines will improve the performance of river conflation.
2. Using Frechet distance could possibly improve Hootenanny river conflation.

=== Power Line Conflation

==== Overview

The goal for the initial conflation of power lines was that the number of correctly conflated features plus the number of features marked 
for manual review would equal at least 80% of the overall conflated features in each test dataset.  An attempt was made to have a as close 
to a minimum of 200 manually matched features as possible in each dataset, while keeping datasets small enough that a single test against 
them could be run in roughly ten minutes or less.  A rules based conflation model was constructed that maximized the conflation output quality
for all input datasets.  

==== Approach

.*Power Line Conflation Attempted Techniques*
[width="100%"]
|======
| *Technique* | *Used in Final Model* | *Description*
| automatic search radius calculation | yes | use rubber sheeting tie points to determine the best conflation search radius
| determine input CE | yes | apply a custom circular error value to each input dataset
| distance weighting | yes | for feature pairs that satisfy the model criterion, favor those that are closer together
| ensemble subline matching | no | attempt to use multiple types of subline matching
| Frechet Distance subline matching | yes | use Frechet Distance subline matching
| geometric based feature extraction | yes | build a rules based model using the geometric properties of the input data
| location tag matching | yes | disambiguate matches using the location of the power line (overhead vs underground)
| rubber sheeting input data | yes | move secondary data toward reference data using rubber sheeting before conflation
| tag based feature extraction | no | build a rules based model using the tag properties of the input data
| voltage tag matching | yes | disambiguate matches using the voltage rating of the power line
|======

===== Subline Matching

Due to the detail in some of the input datasets containing many small power line sections, Frechet Distance based subline matching was the
only available subline matcher that had reasonable runtime performance.  At times, maximal subline matching could catch additional desirable
feature matches that Frechet did not (most of which should be reviewed instead of matched).  However, when attempting to combine the two
within the model, results did not improve and runtime suffered.

===== Rubber Sheeting

Initially, rubber sheeting resulted in runtime exceptions for some of the input data.  The cause for this seems due to multiple ways
sharing the same node, however the exact cause wasn't immediately obvious.  Adding a configuration option to disable the error handling,
turned on by default, for the situation resulted in an increased conflation score without having an adverse affect on any other conflation.

===== Tags

For the input data that contained them, voltage tags were very valuable when trying to disambiguate matches in dense areas of power lines
(especially near power stations).  Fortunately, a lot of the open source data had fairly accurate voltage tags.

To a lesser degree, the location tag used in the OpenStreetMap power line mapping specification was valuable.  Since power lines can exist 
both above and under, it can be difficult to correctly conflate underground power lines if they are not so labeled.

==== Difficulties

The largest difficulty in conflating power lines was due to: 1) differing standards on mapping power lines on towers, 2) data mapped incorrectly based off of aerial imagery, 3) power lines that start as overhead and are later buried underground.  

The OpenStreetMap power line mapping specification states that power towers carrying multiple cables be mapped as
a single way with tags indicating the number of cables carried.  Other datasets (ENERGYDATA.INFO, California State Government) map each 
individual cable as a separate way.  The desired outcome when conflating these two types of data would be to generate a review for user 
to decide which way they want to represent the features.  However, that proved difficult to implement in Hootenanny for this situation and 
needs improvement.

Since power lines can become very dense in urban areas, it can be difficult to correctly map them based off of aerial imagery.  Some of
the OpenStreetMap data used during testing that had been mapped from aerial imagery appeared to incorrectly connect power line ways when
compared to the more specialized open source power line datasets coming from power companies (ENERGYDATA.INFO, California State Government). 

==== Results

.*Power Line Conflation Test Results - July 2018*
[width="100%"]
|======
| *Test Number* | *AOI* | *Reference Data* | *Secondary Data* | *Manually Matched Feature Count* | *Percentage Correctly Conflated* | *Percentage Marked for Unnecessary Review* | *Percentage Combined Correct and Reviewable* | *Time Elapsed (MM:SS)*
| 1 | Mozambique | EDI | OSM | 62 | 96.7 | 0.0 | 96.7 | 05:14
| 2 | Namibia | EDI | OSM | 200 | 51.7 | 0.0 | 51.7 | 23:15
| 3 | California Bay Area | California State Govt | OSM | 229 | 78.4 | 0.0 | 78.4 | 01:39
| 4 | Los Angeles | California State Govt | OSM | 204 | 69.6 | 1.2 | 70.8 | 00:56
| 5 | Namibia | MGCP | EDI | 41 | 71.7 | 0.0 | 71.7 | 00:30 
| 6 | Namibia | EDI | OSM | 51 | 94.7 | 0.0 | 94.7 | 03:53 
|======

The 80% correct conflation threshold was met by two of the tests, with three additional tests within 10% of that value.  The Nambia 
test with only 51.7% correct obviously still requires quite a bit of attention.  

It is worth noting that some of the ENERGYDATA.INFO (EDI) and MGCP data contain previously added OpenStreetMap (OSM) data.  Therefore, in some cases 
nearly identical sections of data are being conflated together, which Hootenanny performs very well against (as expected).  In those areas 
test scores could be considered artificially inflated.  However, since it is a quite common workflow to conflate OpenStreetMap into other 
custom data sources due to OSM's richness as a result of open source contribution, testing conflating such overlapping data is still quite 
valid.

[[GenericLineConflationFutureWork]]
=== Future Work

To investigate this approach and it's performance on known data we will likely
follow the steps listed below. These steps may change with
changing requirements or new knowledge of the problem set and use cases.

1. Identify additional candidates for generic line conflation such as walls or
   railways.
2. Implement routines that give reasonable results based on subjective review.
3. Apply the generic line conflation routines to road training/testing data to
   observe quantitative results.
4. Report findings.

Some areas where the generic line tests above can be improved:

1. Re-run all tests with a single version of the software
2. Address the known Perty limitation bug and re-run the Perty 20m test.
3. Investigate the discrepancy in the manually conflated data sets
4. Extend to include other data types

