== Hootenanny Tests

Hootenanny contains three distinct codebases: the core, the web services, and the user interface.
Each employs a different testing technology, which is described here.

Before building Hootenanny and running its tests, the application must be configured.  See the
Building Hootenanny section in this document for details.

=== Test Suites

Tests are divided into the following groups: current, quick, slow, and glacial.

* **current** - Used when you want to isolate a single test and work on it. No tests should be committed
in this directory.
* **quick** - Individual tests that each run in less than 1 sec.
* **slow** - Individual tests that each run in less than 30 sec.
* **glacial** - Individual tests that each run in less than 15 min.  You shouldn't need unit tests that take longer than 15min.  If you're writing a regression test that uses large input data and runs longer than 15min, then put it with the regression tests, not the unit tests.

If you want to run the tests for all codebases at the same time, first configure as described in a
previous section, then run one of the following commands:

-------------------
make -sj($nproc) test-quick # slow tests
make -sj($nproc) test       # slow tests
make -sj($nproc) test-all   # glacial tests
------------------

"quick" is a subset of the "slow" suite, and "quick" and "slow" are subsets of the "glacial" suite.

As a developer, you are expected to, at a minimum, run the slow tests before pushing commits.
If you are doing user interface or web service work, you may also want to test your changes against
the User Interface Tests, which run as part of the glacial tests.

To see other, more granular testing options, run 'make help'.

=== Core Tests

The Core Tests test everything that is accessible via the Hootenanny command line interface.
The core code is written in C++ and employs several different types of tests, which are described
in the following sections.  The tests are also subdivided into different test suites with differing
runtimes.

In a later section the Hootenanny Web Services Java test code is described.  It is important to note
that there is also C++ code labeled "services".  That is code which writes to the Hoot API and OSM
API services database and is not affiliated with the Hootenanny Web Services Java code.

To run all the Core Tests in a suite, choose one of the following after running configure:

--------------------
HootTest --current
HootTest --quick
HootTest --slow
HootTest --glacial
-------------------

Here are some examples of more granularity when running Core Tests:

---------------
# Runs the glacial tests but excludes any test with 'Big' in the test file name.
HootTest --glacial '--exclude=.*Big.*'

# Runs the glacial tests but only includes tests with 'Big' in the test file name.
HootTest --glacial '--include=.*Big.*'

# Runs a single test
HootTest --single hoot::AlphaShapeTest
--------------

==== Core Unit Tests

The core unit tests use CPPUnit.  The goal is to have one test class for each functional class.
See hoot-core-test/src/test/cpp project for examples.  One CPPUnit test class may contain multiple
CPPUnit tests.

To place a test into one of the four test suites listed in the previous section, a line similar to
the following is placed at the end of the test class:

-----------------
CPPUNIT_TEST_SUITE_NAMED_REGISTRATION(AlphaShapeTest, "quick");
------------------

==== Core Command Line Tests

The C++ portion of Hootenanny exposes functionality via a command line interface. This command line
interface is tested via simple shell scripts. The shell scripts output is compared against a known
good output and these scripts are run via one or more test suites within CPPUnit. The unit tests
and integration tests should provide good coverage of all the code from the command line interface
down.

This is most useful when testing operations that involve multiple parts of the system, or exercise
the command line aspects of the system. Examples include:

* Complex command line options
* Composite operations such as alpha shape, cookie cut, conflate
* Hadoop operations that require moving multiple files to create the test scenario
* Translation scripts

Note that is is generally desirable to make the test do as little as possible to fully exercise
the system. When you write a test it will likely be run thousands of times by a number of people
over its lifetime. No need to make them all wait 20sec for a test that could run in .1sec. It is
likely worth taking an extra 20min to write an efficient test.

**What Do Command Line Tests Do?**

The basic principal of the command line tests is that they verify that the stdout and stderr are
consistent with a given baseline. To do this the following must occur:

* Create an executable script to test some aspect of the system.
* Use the script to generate a first cut output
* Validate the output
* When the output is correct, create a baseline output

When your test runs it will do the following:

* Find all the scripts to execute (the scripts to run are determined by looking for all executable
files in the test-files/cmd/[current|quick|slow|glacial] directories. If the file ends in @.off@ it will be ignored.)
* Run a script and capture the stdout and stderr
* Remove meaningless bits from the stdout and stderr such as INFO statements, DEBUG statements and
the elapsed time print outs.
* Compare the meaningful bits from the test run to the baseline. If there are any difference, report
an error.

**An Example**

To create a test make a shell script that is executable and place it in the
@test-files/cmd/current/@ directory. The shell script should exercise some aspect of the system
and the success/failure should be determined by the output. For instance:

------------
#!/bin/bash

# stop on error
set -e

# Make sure our output directory exists.
mkdir -p test-output/cmd/example

# perform the operation we're testing.
hoot --convert test-files/jakarta_raya_coastline.shp test-output/cmd/example/jakarta.osm

# Write the output to stdout. When this run in the future it'll compare the old output
# to the new input to verify the test is consistent
cat jakarta.osm
------------------

Running HootTest will give an error similar to the one below:

----------------
[hoot2] yubyub:~/dg/src/hoot2$ HootTest --current
.18:27:35.009 WARN  src/main/cpp/hoot/test/ScriptTest.cpp(130) - STDOUT or STDERR don't exist for \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh
*************************
  This can be resolved by reviewing the output for correctness and then
  creating a new baseline. E.g.
  verify:
    less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first
    less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first
  Make a new baseline:
    mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout
    mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr
*************************

F
Failure: /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh
  src/main/cpp/hoot/test/ScriptTest.cpp(138)   - Expression: false
- STDOUT or STDERR does not exist
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh - 0.126008

Elapsed: 0.126034
----------------

As the error message suggests you need to verify the output and then create a new baseline:

-------------
#  verify. Don't skip this!
less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first
less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first
---------------

In this case we goofed in the script and revealed this error in the Example.sh.stderr.first file:

-------------
cat: jakarta.osm: No such file or directory
--------------

Fix the script by changing the last line to:

------------
cat test-output/cmd/example/jakarta.osm
--------------

When you rerun @HootTest --current@ you'll see the .osm file in the .stdout.first file. If
everything looks good create the new baseline.

------------
# Make a new baseline:
mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout
mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr
-------------

Now run the test again and you should get something like:

---------------
[hoot2] yubyub:~/dg/src/hoot2$ HootTest --current
./home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh - 0.146189

Elapsed: 0.146274
-------------

This shows that the test run matches the baseline.

We don't want the test to live in @current@ so we'll move it over to the appropriate test set. In
this case @quick@.

------------
mv test-files/cmd/current/Example* test-files/cmd/quick/
------------

**Inconsistent Output**

Sometimes scripts have output values that change from run to run such as data/time stamps. Many of
these values get stripped out automatically, but if there is something relevant to just your test
you can remove it via grep/sed. If that isn't an option you may need to modify ScriptTest.cpp to
be knowledgeable of your situation. Be careful, because it will modify the way that all tests are
verified.

==== Core Micro Conflate Tests

Frequently it is desirable to test one aspect of the conflation routines. E.g. did the names get
merged properly? Did two buildings get matched/merged? etc. The micro conflate tests are designed
to help with this. These are not, "Did it conflate all of DC exactly the same?" tests or "Did
these 15 roads get conflated properly?" tests. They're intended to test one situation for
correctness. Primarily they're tiny so they don't all break constantly, and it is very easy to
determine what happened.

These tests are discovered/created from directories. For now, only one directory is searched
for tests @test-files/cases/unifying/@. The test creation process goes as follows:

* Search @test-files/cases/unifying@ for a config file (@Config.conf@), if there is one, push it
onto the config file stack.
* If there are directories, recursively search them for tests, but ignore any directories that end
with @.off@
* If there are no directories, search for @Input1.osm@, @Input2.osm@ and @Expected.osm@, if they're
found then create a new test case for this directory.

When a test runs it runs as follows:

* Load all the config files in turn starting with the highest level directory config file.
* Verify that the test has all the required files.
* Run the equivalent of a "--unify" command on the two input files and put the result in @Output.osm@.
* Verify that @Expected.osm@ matches @Output.osm@.

This approach makes it very fast/easy to create new micro tests and run them with the rest of the
test routines. At this time the micro tests run as part of _quick_ and up.

==== Core Plugins Tests

The Plugins Test test various translation related operations.  They may be invoked in isolation with:

--------------
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure
make -sj($nproc) plugins-test
--------------

They run by default in the slow test suite.

==== Core Pretty Pipes Tests

The Pretty Pipes Test test the pretty-pipes submodule.  They may be invoked in isolation with:

--------------
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure
make -sj($nproc) pp-test
--------------

They run by default in the quick test suite.

=== Web Services Tests

The Web Services tests test the Hootenanny web services interface.  There are two types of
Hootenanny web services tests.  One type is written in Java and use JUnit, Jersey, and a
combination of Mockito, PowerMock, EasyMock for mock objects.  One JUnit test class may contain
multiple JUnit tests.  The other type is written in Javascript and uses a combination of mocha and
chai for testing.

It is important to note that there is also C++ code labeled "services".  That is code which writes
to the Hoot API and OSM API services database and is not affiliated with the Hootenanny Web
Services Java code.

==== Test Suites

Java web services test methods may be placed into either the UnitTest or IntegrationTest categories.
The UnitTest suite corresponds to the slow test suite in the Core Tests, and the IntegrationTest
suite corresponds to the glacial test suite.

To run web services unit tests:

---------
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure  --with-services
make -sj($nproc) test
-----------

To run both web services unit and integration tests:

---------
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure  --with-services
make -sj($nproc) test-all
-----------

The above commands will run the corresponding Core Tests immediately after the web services test
complete.  There currently is no means to separate the two.

The mocha based web services tests (see node-export-server/test as an example) are not
currently aligned with the test suites.

===== Java Web Services Unit Tests

The Web Services Unit Tests are meant to test the Java web service code at the class level.
See hoot-services/src/test/java for test examples.

To mark a web service test method as a Unit Test, place the following annotation in front of the
method declaration:

-------------
@Test
@Category(UnitTest.class)
-----------

Unfortunately, we do have quite a few Web Services Tests labeled as Unit Tests which are
technically Integration Tests, since they involve Jersey and Postgres (e.g. MapResourceTest).
The decision was made to leave these are Unit Tests, since they are critical and should be run
with each commit push as part of the slow tests, but those tests should eventually be moved to
the Integration Tests suite and corresponding class level Unit Tests written for them.

===== Java Web Services Integration Tests

The Web Services Integration Tests are meant to test the Java web service code across logical
boundaries, such as HTTP, Postgres, OGC, etc.  See hoot-services/src/test/java for test examples.

To mark a web service test method as a Integration Test, place the following annotation in front
of the method declaration:

-------------
@Test
@Category(IntegrationTest.class)
-----------

===== Javascript Web Services Unit Tests

These test may be run by entering the directly containing the test .js file and running:

---------------
npm install
npm test
---------------

See node-export-server/test as an example.

==== nodejs System Tests (legacy)

Of note, are a set of nodejs system tests which still run as part of the nightly regression testing.  These could be converted to Cucumber user interface tests at some point.

=== User Interface Tests

The User Interface tests come in two types.  The first type uses Cucumber to test the functionality
of the Hootenanny iD browser based application and its interactions with the Hootenanny Web Services.
The second type uses mocha to test at a more granular level.  Of the two, to date more attention
has been paid to the Cucumber tests while the mocha user interface tests do not receive much
attention and may be candidates for removal.

==== Cucumber User Interface Tests

The purpose of these tests is to catch relatively simple errors that get introduced into UI workflows inadvertently, and not to be a bulletproof set of tests for the user interface.  Achieving such a thing really isn't feasible.  Also, since these tests exercise code in all three Hootenanny codebases, they can quickly reveal inconsistencies between both what the web services expect the command line API to be and what it actually is and what the user interface expects the web service API to be and what it actually is.  With this set of tests in place to catch basic errors, we can allow testers to spend more time testing complicated conflation scenarios instead of, for example, waiting for a typo on a single line of code to be fixed before they can complete regression testing.

https://cukes.info[Cucumber] is the technology used to simulate browser interactions in the tests.
https://cukes.info[Cucumber] is the top level interpreter of the
"gherkin language":https://github.com/cucumber/cucumber/wiki/Gherkin that describes each test.
There are many
https://github.com/cucumber/cucumber/wiki/Tutorials-and-Related-Blog-Posts[good tutorials] on the
web to get you started,

* Hootenanny Cucumber User Interface Tests can be found in test-files/ui.
* Cucumber settings may be changed in @features/support/env.rb@.
* Place common test methods in
@features/conflate.feature@ and @features/step_definitions/custom_steps.rb@.
* Each piece of functionality being tested should be placed into its own *.feature file.
* When running silent mode ('make -s'), Cucumber output will be written to
@test-files/ui/tmp/TestRun.log@.  When running without silent mode, Cucumber test output is written
to the screen.

The User Interface Tests run as part of the glacial test suite by default.  You must start Tomcat and then deploy the Hootenanny web services and user interface code to Tomcat yourself before running these tests, as shown below.

To run the User Interface tests with all other glacial tests:

-----------------------
cd $HOOT_HOME
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure --with-services --with-uitests
make -sj($nproc)
sudo -u tomcat6 scripts/CopyWebAppsToTomcat.sh
make -sj($nproc) test-all
-----------------------

To run the User Interface Tests by themselves:

-----------------------
cd $HOOT_HOME
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure --with-services --with-uitests
make -sj($nproc)
sudo -u tomcat6 scripts/CopyWebAppsToTomcat.sh
make -sj($nproc) ui-test
-----------------------

You have to add the --with-services option since the UI tests rely on the services to be deployed before they're run.  The tests will fail with an error message otherwise.

If a test errors out, you'll see the error message on the screen if you're not running in silent mode (-s).  If you are running the tests in silent mode, then you can look in test-files/ui/tmp for the error log.  Browser screenshots should also get written out in the tests directory when tests fail, if that's helpful.

When writing tests, try to avoid creating test scenarios that are likely to change over time due to changes in other parts of the hoot code.  e.g. A test that expects an exact number of reviews from a conflation job.  However, this type of thing is not always completely possible to avoid in order to write good tests.

If you work consistently in the hootenanny-id submodule, then you need to pay close attention to these tests.  If you don't work much in in the hootenanny-id submodule, then it is still possible you can break these tests with changes to either hoot-core/hoot-services code but less likely.

==== Mocha User Interface Tests

These tests reside in hoot-ui/test/spec/hoot and are not known to be currently supported.  They
may be run with:

---------------
cd hoot-ui/test/spec/hoot
npm install
npm test
---------------

=== iD Editor Tests

Although outside of the scope of Hootenanny code, of note for diagnostic purposes are the iD Editor
unit tests.  These can be found in hoot-ui/test/spec.

=== Smoke Tests

The Smoke Tests are manual tests run against the Hootenanny iD browser based application to verify
the results of a Hootenanny installation.  The Smoke Test steps are located here (TODO: fill in
location).

=== Regression Tests

The Regression Tests run Hootenanny command line operations against specific datasets to measure
Hootenanny performance against particular scenarios.  The tests are run against non-public data and,
therefore, are kept in a private DigitalGlobe repository and run on a nightly basis only.  For
more information about the tests, contact hootenanny.help@digitalglobe.com

If you have access to the regression test repository and wish to run them locally, contact
hootenanny.help@digitalglobe.com for further instructions.

Many of the regression tests score Hootenanny's conflation accuracy on a dataset and mark the test
as passing or failing based on an allowable score range.  See
hoot-tests/release_test.child/jakarta-spaghetti.release as an example.

=== Load Tests

The Load Tests test the scalability of the Hootenanny web services code and are run as part of the
nightly tests in a private DigitalGlobe repository.  These tests currently are not meant to be run
in a local development environment.

When run, the tests output an image with graph metrics on Hootenanny scalability for increasing
levels of simulated users.  Here is an example set of test metrics:

image::developer/images/LoadLatest.png[]

=== Test Coverage Reporting

Reports can be generated which detail how well test coverage is for Hootenanny code.  The report
will take into account command line and CPPUnit tests in the Hootenanny core code, as well as Java
JUnit tests in the Hootenanny Web Services code when the application is configured --with-services.
Hootenanny Core code coverage is supplied @gcov@ and @lcov@, utilities for using GCC to generate
coverage results.  Java code coverage is supplied by http://cobertura.github.io/cobertura/[Cobertura]
via Maven.  Report coverage generation for Javascript code is currently in the works.

*Coverage report generation has the pre-requisite of running all tests associated with the code being profiled.*
For the Java Web Services code, this is done automatically by Cobertura, but for the Hootenanny Core code, 'make test' must
be made explicit as described below.

To generate a report for just the Hootenanny Java web services code:

----------------
./configure --with-services --with-coverage && make services-clean-coverage && make -j$(nproc) services-coverage
----------------

To generate a report for all Hootenanny Core code and the Hootenanny Java Web Services code together:

----------
./configure --with-rnd --with-services --with-coverage && make clean-coverage && make -j$(nproc) && make -j$(nproc) test-all && make -j$(nproc) coverage
-----------

The test coverage reports will be output to _$HOOT_HOME/coverage_.

*NOTES:*

* See ngageoint/hootenanny#604 for why the extra 'make' is required before running 'make coverage' when generating a report for the Hootenanny core code.
* To get a code coverage report for *all* Hootenanny code, you *must* run 'make test-all' before running 'make coverage'
for the Hootenanny Core code or you will get incomplete results.  The Java Web Services'make services-coverage' command doesn't actually require 'make test' to be run
beforehand since it is done automatically, but if you run 'make coverage' when generating coverage reporting using the --with-services configuration option,
you should always run 'make test' beforehand in order to not receive inaccurate reporting for the Hootenanny Core code.
* For the Hootenanny Core code coverage reporting, although certain configuration options disable the compiling of certain code (--with-rnd, --with-services, etc.), the total lines the coverage report reports does not seem to change.  Therefore, to get a totally accurate coverage report for the Core code you need to always run with all configuration options enabled and run 'make test-all' to make sure all the code is covered.
* See ngageoint/hootenanny#610 for why the Java web services tests will run twice when generating the Hootenanny Core and Web Services coverage reports in the same command.

=== General Test Writing Guidelines

* Unit tests should strive to test at the single class level only, when possible.
* Unit tests should have nearly a one to one mapping to each class in code.  Use the code coverage
report to see where your tests are deficient.
* Unit tests should avoid interfacing with external entities, when possible. e.g. databases,
web servers (Note: Many of the Java services tests violate this and should be updated).  Such
tests that interface with external entities should then become integration tests instead.
* Unit tests should cover as many exceptional error handling cases as is reasonable.
* Use clear test method names to state what you are testing.
* Make gratuitous use of asserts during testing.
* Use comments in test methods where its not obvious in the code how/why you're testing something.
* Care should be taken to categorize tests based on the amount of time they complete. e.g. don't
put a longer running test in the C++ quick tests.  For the Java tests, longer running tests should
most likely be put into the integration tests.
* Small amounts of test data should be used for testing if possible.  Do not check large test data
files into the repository.
* During testing you can verify test output via asserts against the state of the output data or via
file comparison of the output with known good output.  An advantage to using file comparison for
testing output is that the test code is less verbose and tedious to maintain as the class evolves.
A disadvantage of using file comparison is that it is not always clear what the intentions of your
test are and individuals can inadvertently overwrite your intended test output if they do not
understand why they broke the test.  Weigh these pros and cons when selecting which one of these
test output verification methods you will use.
* Do not overwrite generated test output used to verify a test unless you are sure that in doing
so you are still preserving the integrity of the test.
* In Java, mock objects are your friend when writing tests.
* Design a class so that testing of all of its members is possible.  In some cases, you may need
to expose members only to the tests.  e.g. Use C++ friend keyword, etc.; or in Java, Mockito may
help with this.


