
== Hootenanny Tests

Hootenanny contains three distinct codebases: the core, the web services, and the user interface.
Each employs a different testing technologies, which are described here. Before building Hootenanny and 
running its tests, the application must be configured. See the Building Hootenanny section in this 
document for details.

=== Test Suites

Tests are divided into the following groups: current, quick, slow, and glacial.

* **current** - Used when you want to isolate a single test and work on it. No tests should be committed
in this directory.
* **quick** - Individual tests that each run in less than 1 sec.
* **slow** - Individual tests that each run in less than 30 sec.
* **glacial** - Individual tests that each run in less than 15 min. You shouldn't need unit tests that take longer than 15min. If you're writing a regression test that uses large input data and runs longer than 15min, then put it with the regression tests, not the unit tests.  Also, hoot-rnd tests usually get placed in this suite, since they employ non-production based functionality that changes less frequently than code in hoot-core.

If you want to run the tests for all codebases at the same time, first configure as described in a
previous section, then run one of the following commands:

-------------------
make -sj$(nproc) test-quick # quick tests
make -sj$(nproc) test       # slow tests
make -sj$(nproc) test-all   # glacial tests
------------------

"quick" is a subset of the "slow" suite, and "quick" and "slow" are subsets of the "glacial" suite.

To see other, more granular testing options, run 'make help'.

=== Core Tests

The Core Tests test everything that is accessible via the Hootenanny command line interface.
The core code is written in C++ and employs several different types of tests, which are described
in the following sections. The tests are also subdivided into different test suites with differing
runtimes.

In a later section the Hootenanny Web Services Java test code is described.  It is important to note
that there is also C++ code labeled "services".  That is code which writes to the Hoot API and OSM
API services database and is not affiliated with the Hootenanny Web Services Java code.

To run all the Core Tests in a suite, choose one of the following after running configure:

--------------------
HootTest --current
HootTest --quick
HootTest --case-only
HootTest --slow
HootTest --slow-only
HootTest --glacial
HootTest --glacial-only
-------------------

Here are some examples of more granularity when running Core Tests:

---------------
# Runs the glacial tests but excludes any test with 'Big' in the test file name.
HootTest --glacial '--exclude=.*Big.*'

# Runs the glacial tests but only includes tests with 'Big' in the test file name.
HootTest --glacial '--include=.*Big.*'

# Runs a single test
HootTest --single hoot::AlphaShapeTest
---------------

==== Core Unit Tests

The core unit tests use CPPUnit. The goal is to have one test class for each functional class, but
in many cases the coverage from the command line tests (described later) preven this from being 
necessary. See hoot-core-test/src/test/cpp project for examples. One CPPUnit test class may contain 
multiple CPPUnit tests.

To place a test into one of the four test suites listed in the previous section, a line similar to
the following is placed at the end of the test class:

-----------------
CPPUNIT_TEST_SUITE_NAMED_REGISTRATION(AlphaShapeTest, "quick");
-----------------

==== Core Command Line Tests

The C++ portion of Hootenanny exposes functionality via a command line interface. This command line
interface is tested via simple shell scripts. The shell scripts output is compared against a known
good output and these scripts are run via one or more test suites within CPPUnit. The unit tests
and integration tests should provide good coverage of all the code from the command line interface
down.

This is most useful when testing operations that involve multiple parts of the system, or exercise
the command line aspects of the system. Examples include:

* Complex command line options
* Composite operations such as alpha shape, cookie cut, conflate
* Translation scripts

Note that is is generally desirable to make the test do as little as possible to fully exercise
the system. When you write a test it will likely be run thousands of times by a number of people
over its lifetime. No need to make them all wait 20sec for a test that could run in .1sec. It is
likely worth taking an extra 20min to write an efficient test.

**What Do Command Line Tests Do?**

The basic principal of the command line tests is that they verify that the stdout and stderr are
consistent with a given baseline. To do this the following must occur:

* Create an executable script to test some aspect of the system.
* Use the script to generate a first cut output
* Validate the output
* When the output is correct, create a baseline output

When your test runs it will do the following:

* Find all the scripts to execute (the scripts to run are determined by looking for all executable
files in the test-files/cmd/[current|quick|slow|glacial] directories. If the file ends in +.off+ it will be ignored.)
* Run a script and capture the stdout and stderr
* Remove meaningless bits from the stdout and stderr such as INFO statements, DEBUG statements and
the elapsed time print outs.
* Compare the meaningful bits from the test run to the baseline. If there are any difference, report
an error.

**An Example**

To create a test make a shell script that is executable and place it in the
+test-files/cmd/current/+ directory. The shell script should exercise some aspect of the system
and the success/failure should be determined by the output. For instance:

------------
#!/bin/bash

# stop on error
set -e

# Make sure our output directory exists.
mkdir -p test-output/cmd/example

# perform the operation we're testing.
hoot convert test-files/jakarta_raya_coastline.shp test-output/cmd/example/jakarta.osm

# Write the output to stdout. When this run in the future it'll compare the old output
# to the new input to verify the test is consistent
cat jakarta.osm
------------

Running HootTest will give an error similar to the one below:

----------------
[hoot2] yubyub:~/dg/src/hoot2$ HootTest --current
.18:27:35.009 WARN  src/main/cpp/hoot/test/ScriptTest.cpp(130) - STDOUT or STDERR don't exist for \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh
*************************
  This can be resolved by reviewing the output for correctness and then
  creating a new baseline. E.g.
  verify:
    less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first
    less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first
  Make a new baseline:
    mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout
    mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr
*************************

F
Failure: /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh
  src/main/cpp/hoot/test/ScriptTest.cpp(138)   - Expression: false
- STDOUT or STDERR does not exist
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh - 0.126008

Elapsed: 0.126034
----------------

As the error message suggests you need to verify the output and then create a new baseline:

-------------
#  verify. Don't skip this!
less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first
less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first
-------------

In this case we goofed in the script and revealed this error in the Example.sh.stderr.first file:

-------------
cat: jakarta.osm: No such file or directory
-------------

Fix the script by changing the last line to:

------------
cat test-output/cmd/example/jakarta.osm
------------

When you rerun +HootTest --current+ you'll see the .osm file in the .stdout.first file. If
everything looks good create the new baseline.

------------
# Make a new baseline:
mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout
mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr
------------

Now run the test again and you should get something like:

-------------
[hoot2] yubyub:~/dg/src/hoot2$ HootTest --current
./home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh - 0.146189

Elapsed: 0.146274
-------------

This shows that the test run matches the baseline.

We don't want the test to live in +current+ so we'll move it over to the appropriate test set. In
this case +quick+.

------------
mv test-files/cmd/current/Example* test-files/cmd/quick/
------------

**Inconsistent Output**

Sometimes scripts have output values that change from run to run such as data/time stamps. Many of
these values get stripped out automatically, but if there is something relevant to just your test
you can remove it via grep/sed. If that isn't an option you may need to modify ScriptTest.cpp to
be knowledgeable of your situation. Be careful, because it will modify the way that all tests are
verified.

==== Core Micro Conflate Tests (Case Tests)

Frequently it is desirable to test one aspect of the conflation routines. E.g. did the names get
merged properly? Did two buildings get matched/merged? etc. The micro conflate tests are designed
to help with this. These are not, "Did it conflate all of DC exactly the same?" tests or "Did
these 15 roads get conflated properly?" tests. They're intended to test one situation for
correctness. Primarily they're tiny so they don't all break constantly, and it is very easy to
determine what happened.

These tests are discovered/created from +test-files/cases+. The test creation process goes as follows:

* Search +test-files/cases+ for a config file (+Config.conf+), if there is one, push it
onto the config file stack.
* If there are directories, recursively search them for tests, but ignore any directories that end
with +.off+
* If there are no directories, search for +Input1.osm+, +Input2.osm+ and +Expected.osm+, if they're
found then create a new test case for this directory.

When a test runs it runs as follows:

* Load all the config files in turn starting with the highest level directory config file.
* Verify that the test has all the required files.
* Run the equivalent of a conflate command on the two input files and put the result in +Output.osm+.
* Verify that +Expected.osm+ matches +Output.osm+.

This approach makes it very fast/easy to create new micro tests and run them with the rest of the
test routines. At this time the micro tests run as part of _quick_ and up.

To only run case tests execute:

------------
HootTest --case-only
------------

See +test-files/cases/README+ for additional information on case tests.

==== Running Core Unit Tests in Parallel

Hootenanny can run certain unit tests in parallel.  This is accomplished by `--parallel [n]` flag
where the optional `[n]` specifies the number of worker processes to spawn.  Leaving off the `[n]`
parameter makes an implicit call to the operating system to get the total number of online processing units.
(The same as +$(nproc)+ )

--------------
# Runs quick tests serially
HootTest --quick

# Runs quick tests with two parallel processes
HootTest --quick --parallel 2

# Runs quick tests in parallel with one process per processing unit
HootTest --quick --parallel $(nproc)
# Or implicit call to nproc
HootTest --quick --parallel
--------------

Initial testing shows using `$(nproc)` or leaving the parameter empty is the optimal setting for speed
as any more than that causes processes to wait for significantly longer for CPU time and give no real benefit.

These worker processes are QProcess objects that spawn `HootTest --listen`.  This "listening" process
accepts single unit test names (similar to `--single`) from standard in, runs the test and then sends
an end-of-test output command.  The master process listens for this command and once received it sends
another single unit test from the queue.  Once the queue is empty, the master process shuts down the
listen processes and ends.

Some tests that use particular database tables and users have to run serially or they will fail so all
of the code-based tests are marked and added to a `serial` test suite.  Those tests are also added to
their respective test suite based on complexity and time (i.e. `quick`, `slow` or `glacial`).

-----------------
CPPUNIT_TEST_SUITE_NAMED_REGISTRATION(GlacialUnitTestThatMustBeRunSeriallyTest, "serial");
CPPUNIT_TEST_SUITE_NAMED_REGISTRATION(GlaicalUnitTestThatMustBeRunSeriallyTest, "glacial");
-----------------

Core command line tests, or script-based tests, that can be run in parallel are located in
`$HOOT_HOME/tests-files/cmd/slow/` and `$HOOT_HOME/test-files/cmd/glacial/`.  Those command line tests
that must be run serially are in a sub-directory under those locations, i.e.
`$HOOT_HOME/test-files/cmd/slow/serial/` and `$HOOT_HOME/test-files/cmd/glacial/serial/`.

Serial unit tests are all passed off to the first worker process and run serially while the rest of the
tests are run on the other parallel processes.  Once the first worker process completes the serial
tests it will continue to help the other processes with the parallel queue.

NOTE:  While creating new unit tests every effort should be made to allow for the tests to be run
in parallel.  Do not reuse output filenames to avoid stomping on files.  Input files can be reused
though.

==== Core Plugins Tests

The Plugins Test test various translation related operations.  They may be invoked in isolation with:

--------------
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure
make -sj$(nproc) translations-test
--------------

To run an individual test:
--------------
cd $HOOT_HOME/translations/test
mocha <test name>.js
--------------

=== Web Services Tests

The Web Services tests test the Hootenanny web services interface.  There are two types of
Hootenanny web services tests.  One type is written in Java and use JUnit, Jersey, and a
combination of Mockito, PowerMock, EasyMock for mock objects.  One JUnit test class may contain
multiple JUnit tests.  The other type is written in Javascript and uses a combination of mocha and
chai for testing.

It is important to note that there is also C++ code labeled "services".  That is code which writes
to the Hoot API and OSM API services database and is not affiliated with the Hootenanny Web
Services Java code.

==== Test Suites

Java web services test methods may be placed into either the UnitTest or IntegrationTest categories.
The UnitTest suite corresponds to the slow test suite in the Core Tests, and the IntegrationTest
suite corresponds to the glacial test suite.

To run web services unit tests:

---------
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure --with-services
make -sj$(nproc) test
---------

To run both web services unit and integration tests:

---------
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure --with-services
make -sj$(nproc) test-all
---------

The above commands will run the corresponding Core Tests immediately after the web services test
complete.  There currently is no means to separate the two.

The mocha based web services tests (see node-export-server/test as an example) are not
currently aligned with the test suites.

===== Java Web Services Unit Tests

The Web Services Unit Tests are meant to test the Java web service code at the class level.
See hoot-services/src/test/java for test examples.

To mark a web service test method as a Unit Test, place the following annotation in front of the
method declaration:

-----------
@Test
@Category(UnitTest.class)
-----------

===== Java Web Services Integration Tests

The Web Services Integration Tests are meant to test the Java web service code across logical
boundaries, such as HTTP, Postgres, OGC, etc.  See hoot-services/src/test/java for test examples.

To mark a web service test method as a Integration Test, place the following annotation in front
of the method declaration:

-----------
@Test
@Category(IntegrationTest.class)
-----------

Unfortunately, we do have quite a few Web Services Tests labeled as Unit Tests which are
technically Integration Tests, since they involve Jersey and Postgres (e.g. MapResourceTest).
The decision was made to leave these are Unit Tests, since they are critical and should be run
with each commit push as part of the slow tests, but those tests should eventually be moved to
the Integration Tests suite and corresponding class level Unit Tests written for them.

=== User Interface Tests

The User Interface tests come in two types. The first type uses Cucumber to test the functionality
of the Hootenanny iD browser based application and its interactions with the Hootenanny Web Services.
The second type uses mocha to test at a more granular level.

==== Cucumber User Interface Tests

The purpose of these tests is to catch relatively simple errors that get introduced into UI workflows inadvertently, and not to be a bulletproof set of tests for the user interface. Achieving such a thing really isn't feasible.  Also, since these tests exercise code in all three Hootenanny codebases, they can quickly reveal inconsistencies between both what the web services expect the command line API to be and what it actually is and what the user interface expects the web service API to be and what it actually is. With this set of tests in place to catch basic errors, we can allow testers to spend more time testing complicated conflation scenarios instead of, for example, waiting for a typo on a single line of code to be fixed before they can complete regression testing.

link:$$https://cukes.info$$[Cucumber] is the technology used to simulate browser interactions in the tests.
Cucumber is the top level interpreter of the
link:$$https://github.com/cucumber/cucumber/wiki/Gherkin$$[gherkin language] that describes each test.
There are many
link:$$https://github.com/cucumber/cucumber/wiki/Tutorials-and-Related-Blog-Posts$$[good tutorials] on the
web to get you started,

* Hootenanny Cucumber User Interface Tests can be found in test-files/ui.
* Cucumber settings may be changed in `features/support/env.rb`.
* Place common test methods in `features/conflate.feature` and `features/step_definitions/custom_steps.rb`.
* Each piece of functionality being tested should be placed into its own *.feature file.
* When running silent mode (+make -s+), Cucumber output will be written to
`test-files/ui/tmp/TestRun.log`.  When running without silent mode, Cucumber test output is written
to the screen.

The User Interface Tests run as part of the glacial test suite by default. You must start Tomcat and then deploy the Hootenanny web services and user interface code to Tomcat yourself before running these tests, as shown below.

To run the User Interface tests with all other glacial tests:

-----------------------
cd $HOOT_HOME
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure --with-services --with-uitests
make -sj$(nproc)
sudo -u tomcat8 scripts/tomcat/CopyWebAppsToTomcat.sh
make -sj$(nproc) test-all
-----------------------

To run the User Interface Tests by themselves:

-----------------------
cd $HOOT_HOME
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure --with-services --with-uitests
make -sj$(nproc)
sudo -u tomcat8 scripts/tomcat/CopyWebAppsToTomcat.sh
make -sj$(nproc) ui-test
-----------------------

You have to add the --with-services option since the UI tests rely on the services to be deployed before they're run. The tests will fail with an error message otherwise.

If a test errors out, you'll see the error message on the screen if you're not running in silent mode (-s).  If you are running the tests in silent mode, then you can look in test-files/ui/tmp for the error log. Browser screenshots should also get written out in the tests directory when tests fail, if that's helpful.

When writing tests, try to avoid creating test scenarios that are likely to change over time due to changes in other parts of the hoot code.  e.g. A test that expects an exact number of reviews from a conflation job.  However, this type of thing is not always completely possible to avoid in order to write good tests.

==== Mocha User Interface Tests

These tests reside in hoot-ui/test/spec/hoot and are not known to be currently supported.  They
may be run with:

---------------
cd hoot-ui/test/spec/hoot
npm install
npm test
---------------

=== Regression Tests

The Regression Tests run Hootenanny command line operations against specific datasets to measure
Hootenanny performance against particular scenarios.  The tests are run against non-public data and,
therefore, are kept in a private Maxar repository and run on a nightly basis only. For
more information about the tests, create an issue at https://github.com/ngageoint/hootenanny.

Many of the regression tests score Hootenanny's conflation accuracy on a dataset and mark the test
as passing or failing based on an allowable score range.  See
hoot-tests/release_test.child/jakarta-spaghetti.release as an example.

=== Load Tests

NOTE: The load tests have been inactive for awhile with no current plans to revive them.

The Load Tests test the scalability of the Hootenanny web services code and are run as part of the
nightly tests in a private Maxar repository. These tests currently are not meant to be run
in a local development environment. When run, the tests output an image with graph metrics on 
Hootenanny scalability for increasing levels of simulated users.  Here is an example set of test 
metrics:

image::images/LoadLatest.png[]

=== Test Coverage Reporting

Reports can be generated which detail the test coverage of Hootenanny code from the command line.
Hootenanny Core code coverage is supplied +gcov+ and +lcov+. Java code coverage is supplied by 
http://cobertura.github.io/cobertura/[Cobertura] via Maven.  

Alternatively, you can configure your Jenkins pull request job to generate a test coverage 
report (core only) by selecting: "Build with Parameters" and checking the "Core_coverage" and 
"Configure_coverage" checkboxes. The coverage report can be downloaded from the Jenkins job page as 
`coverage.tar.gz`.

*Coverage report generation has the pre-requisite of running all tests associated with the code 
being profiled.* For the Java Web Services code, this is done automatically by Cobertura, but for 
the Hootenanny Core code, 'make test' must be made explicit as described below.

To generate a report for just the Hootenanny Core code:

----------------
./configure --with-rnd --with-josm --with-services --with-coverage && make clean-coverage && make -j$(nproc) && HootTest --glacial --parallel && make -j$(nproc) core-coverage
----------------

To generate a report for just the Hootenanny Java web services code:

----------------
./configure --with-services --with-coverage && make services-clean-coverage && make -j$(nproc) services-coverage
----------------

To generate a report for all Hootenanny Core code and the Hootenanny Java Web Services code together:

-----------
./configure --with-rnd --with-josm --with-services --with-coverage && make clean-coverage && make -j$(nproc) && make -j$(nproc) test-all && make -j$(nproc) coverage
-----------

The test coverage reports will be output to _$HOOT_HOME/coverage_.

*NOTES:*

* Running tests `--with-coverage` may signficantly slow down their runtime and increase their memory
usage. Therefore, its recommended to use `--with-coverage` only when you need to generate a test
coverage report, and you may need to use a lower `--parallel` test count in conjunction with it.
* To get a code coverage report for *all* Hootenanny code, you *must* run 'make test-all' before 
running 'make coverage' for the Hootenanny Core code or you will get incomplete results. The Java 
Web Services 'make services-coverage' command doesn't actually require 'make test' to be run 
beforehand since it is done automatically, but if you run 'make coverage' when generating coverage 
reporting using the `--with-services` configuration option, you should always run 'make test' 
beforehand in order to not receive inaccurate reporting for the Hootenanny Core code.
* For Hootenanny Core code coverage reporting, to get a totally accurate coverage report you need to 
run with all compile configuration options enabled (--with-rnd, --with-services, etc.), and either 
run 'make test-all', or if you don't want to run the Java web services tests then run something 
like: 'HootTest --glacial --parallel && make translations-test'.

=== General Test Writing Guidelines

* Unit tests should strive to test at the single class level only, when possible.
* Unit tests should have nearly a one to one mapping to each class in code. Use the code coverage
report to see where your tests are deficient.
* Unit tests should avoid interfacing with external entities, when possible. e.g. databases,
web servers (Note: Many of the Java services tests violate this and should be updated).  Such
tests that interface with external entities should then become integration tests instead.
* Unit tests should cover as many exceptional error handling cases as is reasonable.
* Use clear test method names to state what you are testing.
* Make gratuitous use of asserts during testing.
* Use comments in test methods where its not obvious in the code how/why you're testing something.
* Care should be taken to categorize tests based on the amount of time they complete. e.g. don't
put a longer running test in the C++ quick tests. For the Java tests, longer running tests should
most likely be put into the integration tests.
* Small amounts of test data should be used for testing if possible. Do not check very large test data
files into the repository. Keep test files to <= 1MB when possible.
* During testing you can verify test output via CPPUnit asserts against the state of the output data or via
file comparison (`HOOT_FILE_EQUALS`) of the output with known good output. An advantage to using file 
comparison for testing output is that the test code is less verbose and tedious to maintain as the 
class evolves. A disadvantage of using file comparison is that it is not always clear what the 
intentions of your test are and individuals can inadvertently overwrite your intended test output if 
they do not understand why they broke the test. Weigh these pros and cons when selecting which one 
of these test output verification methods you will use.
* Do not overwrite generated test output used to verify a test unless you are sure that in doing
so you are still preserving the integrity of the test.
* In Java, mock objects are your friends when writing tests.
* Design a class so that testing of all of its members is possible. In some cases, you may need
to expose members only to the tests. e.g. Use C++ friend keyword, etc.; or in Java, Mockito may
help with this.

=== Testing with Address Data

Any tests that manipulate address data must use the third party library, libpostal. libpostal 
requires a large amount of data at initialization time and is treated as a Singleton with the 
`LibPostalInit` class in code to ensure only loading that data once. When writing tests that call 
`LibPostalInit`, additional care must taken to make sure it is only being initialized once during 
multi-threaded test execution. Each test process thread potentially can call `LibPostalInit` if it 
launches a test that manipulates address data. 

To cut down on calls to `LibPostalInit`, by default address matching is turned of via the option 
`address.match.enabled=false`. The option is to be enabled only for tests that require using address 
data. To absolutely ensure that we are only creating one `LibPostalInit`
instance, all address related tests are executed within a single CPPUnit test method within the 
class `AddressConflateTest` which is executed as part of the serial test queue. This solution is a 
little kludgy, but works. This makes tracking down address related test failures more difficult, but 
it can still be done via the CPPUnit test failure line number reported. No address related tests 
should be created as command line tests, which will run in a separate process each. Also, no address 
related tests should be created as case tests, since there is no mechanism to run them serially.

Going forward there here are some things that may make usage of libpostal in tests easier:

* Several of the tests using `LibPostalInit` could possibly be mocked to reduce
potential calls to it. However, Hootenanny currently does not have a mock testing framework 
available. 
* The Hootenanny test framework could possibly be manipulated to ensure all address related tests
run within the same process via some address test registry mechanism. That would obviate the need
for the combined `AddressConflateTest`. (related: #4908)
* Possibly there is a way to load and work with a reduced subset of libpostal data for testing 
purposes.


