
=== Hootenanny Tests

Hootenanny contains three distinct codebases: the core, the web services, and the user interface.
Each employs a different testing technologies, which are described here. Before building Hootenanny 
and running its tests, the application must be configured. See the Building Hootenanny section in 
this document for details.

==== Test Suites

Tests are divided into the following groups: current, quick, slow, and glacial.

* **current** - Used when you want to isolate a single test and work on it. No tests should be 
committed in this directory.
* **quick** - Individual tests that each run in less than 1 sec.
* **slow** - Individual tests that each run in less than 30 sec.
* **glacial** - Individual tests that each run in less than 15 min. You shouldn't need unit tests 
that take longer than 15min. If you're writing a regression test that uses large input data and runs 
longer than 15min, then put it with the regression tests, not the unit tests.

If you want to run the tests for all codebases at the same time, first configure as described in a
previous section, then run one of the following commands:
-------------------
make -sj$(nproc) test-quick # quick tests
make -sj$(nproc) test       # slow tests
make -sj$(nproc) test-all   # glacial tests
------------------

"quick" is a subset of the "slow" suite, and "quick" and "slow" are subsets of the "glacial" suite.

To see other, more granular testing options, run 'make help'.

==== Core Tests

The Core Tests test everything that is accessible via the Hootenanny command line interface.
The core code is written in C++ and employs several different types of tests, which are described
in the following sections. The tests are also subdivided into different test suites with differing
runtimes.

In a later section the Hootenanny Web Services Java test code is described.  It is important to note
that there is also C++ code labeled "services".  That is code which writes to the Hoot API and OSM
API services database and is not affiliated with the Hootenanny Web Services Java code.

To run all the Core Tests in a suite, choose one of the following after running configure:
--------------------
HootTest --current
HootTest --quick
HootTest --case-only
HootTest --slow
HootTest --slow-only
HootTest --glacial
HootTest --glacial-only
-------------------

Here are some examples of more granularity when running Core Tests:
---------------
# Runs the glacial tests but excludes any test with 'Big' in the test file name.
HootTest --glacial '--exclude=.*Big.*'

# Runs the glacial tests but only includes tests with 'Big' in the test file name.
HootTest --glacial '--include=.*Big.*'

# Runs a single test
HootTest --single AlphaShapeTest
---------------

===== Unit Tests

The core unit tests use CPPUnit. The goal is to have one test class for each functional class, but
in many cases the coverage from the command line tests (described later) prevent this from being 
necessary. See hoot-core-test/src/test/cpp project for examples. One CPPUnit test class may contain 
multiple CPPUnit tests.

To place a test into one of the four test suites listed in the previous section, a line similar to
the following is placed at the end of the test class:
-----------------
CPPUNIT_TEST_SUITE_NAMED_REGISTRATION(AlphaShapeTest, "quick");
-----------------

To ensure a test runs serially, a line similar to the following is placed at the end of the test 
class in addition to the line assigning the test class to a test suite:
-----------------
CPPUNIT_TEST_SUITE_NAMED_REGISTRATION(AlphaShapeTest, "serial");
-----------------

Tests which make use of the Hootenanny or OSM API databases should have their class name prefixed 
with `Service` and also have an include condition added similar to the following:
-----------------
#ifdef HOOT_HAVE_SERVICES
CPPUNIT_TEST_SUITE_NAMED_REGISTRATION(ServiceHootApiDbTest, "slow");
#endif  
-----------------

Due to use of the configuration, some tests may require an environment reset before they can run.
You are encouraged to create your configuration locally within each test method and pass it into the
Unit Under Test (UUT), however sometimes that is not possible. These are the options for resetting
the environment between CPPUnit test runs:

* `ResetBasic` - This resets element ID counters within `OsmMap`, the random number seed generator, 
and the UUID generator. Resetting all of this is generally necessary to compare map output within a 
test. This is the default and does not need to be explicitly specified.
* `ResetConfigs` - In addition to what is reset in `ResetBasic`, this resets the configuration file
settings back to their default values.
* `ResetEnvironment` - In addition to what is reset in `ResetConfigs`, this resets the configuration
along with many other internal settings, the `MergerFactory` and `TagMergerFactory` but does not reset
the `MatchFactory` class (expensive to do in the tests; see code for details). This reset option is
discouraged overall but necessary for some tests.
* `ResetAll` - This does all the resetting done in `ResetEnvironment` and additionally resets the
`MatchFactory` class. This is the most resource expensive reset option.

To specify these reset options within a CPPUnit test, add something similar to the following to your 
test constructor:
-----------
WayJoinerConflateTest() :
HootTestFixture(
  "test-files/algorithms/WayJoinerConflateTest/", "test-output/algorithms/WayJoinerConflateTest/")
{
  setResetType(ResetAll);
}
-----------

Note in the above example that two directories are specified. The first is the directory under 
`HOOT_HOME` where the test's input files reside. The second is the directory under `HOOT_HOME` to
write test output files. If not input and/or output directories are required by the test, you can
use `UNUSED_PATH`, which is the default.

*_Testing with Address Data_*

Any tests that manipulate address data must use the third party library, libpostal. libpostal 
requires a large amount of data at initialization time and is treated as a Singleton with the 
`LibPostalInit` class in code to ensure only loading that data once. When writing tests that call 
`LibPostalInit`, additional care must taken to make sure it is only being initialized once during 
multi-threaded test execution. Each test process thread potentially can call `LibPostalInit` if it 
launches a test that manipulates address data. 

To cut down on calls to `LibPostalInit`, by default address matching is turned of via the option 
`address.match.enabled=false`. The option is to be enabled only for tests that require using address 
data. To absolutely ensure that we are only creating one `LibPostalInit`
instance, all address related tests are executed within a single CPPUnit test method within the 
class `AddressConflateTest` which is executed as part of the serial test queue. This solution is a 
little kludgy, but works. This makes tracking down address related test failures more difficult, but 
it can still be done via the CPPUnit test failure line number reported. No address related tests 
should be created as command line tests, which will run in a separate process each. Also, no address 
related tests should be created as case tests, since there is no mechanism to run them serially.

Going forward there here are some things that may make usage of libpostal in tests easier:

* Several of the tests using `LibPostalInit` could possibly be mocked to reduce
potential calls to it. However, Hootenanny currently does not have a mock testing framework 
available. 
* The Hootenanny test framework could possibly be manipulated to ensure all address related tests
run within the same process via some address test registry mechanism. That would obviate the need
for the combined `AddressConflateTest`. (related: #4908)
* Possibly there is a way to load and work with a reduced subset of libpostal data for testing 
purposes.

*_Additional Details_*

* Tests within the `hoot-rnd` module are generally always run in the `--glacial` test suite.
* Tests which use the single test OSM API database instance must be run in series.

===== Command Line Tests

The C++ portion of Hootenanny exposes functionality via a command line interface. This command line
interface is tested via simple shell scripts. The shell scripts output is compared against a known
good output and these scripts are run via one or more test suites within CPPUnit. The unit tests
and integration tests should provide good coverage of all the code from the command line interface
down.

This is most useful when testing operations that involve multiple parts of the system, or exercise
the command line aspects of the system. Examples include:

* Complex command line options
* Composite operations such as alpha shape, cookie cut, conflate
* Translation scripts

Note that is is generally desirable to make the test do as little as possible to fully exercise
the system. When you write a test it will likely be run thousands of times by a number of people
over its lifetime. No need to make them all wait 20sec for a test that could run in .1sec. It is
likely worth taking an extra 20min to write an efficient test.

Core command line tests may be forced to run in series by placing them in a directory named "serial" 
in their respective test suite directory. e.g. `$HOOT_HOME/tests-files/cmd/slow/serial`. Any test
not in the "serial" subdirectory is eligible for parallel execution.

*_What Do Command Line Tests Do?_*

The basic principal of the command line tests is that they verify that the stdout and stderr are
consistent with a given baseline. To do this the following must occur:

* Create an executable script to test some aspect of the system.
* Use the script to generate a first cut output
* Validate the output
* When the output is correct, create a baseline output

When your test runs it will do the following:

* Find all the scripts to execute (the scripts to run are determined by looking for all executable
files in the test-files/cmd/[current|quick|slow|glacial] directories. If the file ends in `.off` it 
will be ignored.)
* Run a script and capture the stdout and stderr
* Remove meaningless bits from the stdout and stderr such as INFO statements, DEBUG statements and
the elapsed time print outs.
* Compare the meaningful bits from the test run to the baseline. If there are any difference, report
an error.

**An Example**

To create a test make a shell script that is executable and place it in the
`test-files/cmd/current/` directory. The shell script should exercise some aspect of the system
and the success/failure should be determined by the output. For instance:
------------
#!/bin/bash

# stop on error
set -e

LOG_LEVEL="--warn"
CONFIG="-C Testing.conf"

# Make sure our output directory exists.
mkdir -p test-output/cmd/example

# perform the operation we're testing.
hoot convert $LOG_LEVEL $CONFIG test-files/jakarta_raya_coastline.shp test-output/cmd/example/jakarta.osm

# Write the output to stdout. When this run in the future it'll compare the old output
# to the new input to verify the test is consistent
cat jakarta.osm
------------

Running HootTest will give an error similar to the one below:
----------------
[hoot2] yubyub:~/dg/src/hoot2$ HootTest --current
.18:27:35.009 WARN  src/main/cpp/hoot/test/ScriptTest.cpp(130) - STDOUT or STDERR don't exist for \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh
*************************
  This can be resolved by reviewing the output for correctness and then
  creating a new baseline. E.g.
  verify:
    less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first
    less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first
  Make a new baseline:
    mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout
    mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr
*************************

F
Failure: /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh
  src/main/cpp/hoot/test/ScriptTest.cpp(138)   - Expression: false
- STDOUT or STDERR does not exist
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh - 0.126008

Elapsed: 0.126034
----------------

As the error message suggests you need to verify the output and then create a new baseline:
-------------
#  verify. Don't skip this!
less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first
less /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first
-------------

In this case we goofed in the script and revealed this error in the Example.sh.stderr.first file:
-------------
cat: jakarta.osm: No such file or directory
-------------

Fix the script by changing the last line to:
------------
cat test-output/cmd/example/jakarta.osm
------------

When you rerun `HootTest --current` you'll see the .osm file in the .stdout.first file. If
everything looks good create the new baseline.
------------
# Make a new baseline:
mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stdout
mv /home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr.first \
/home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh.stderr
------------

Now run the test again and you should get something like:
-------------
[hoot2] yubyub:~/dg/src/hoot2$ HootTest --current
./home/jason.surratt/dg/src/hoot2/hoot-core/src/test/resources/cmd/current/Example.sh - 0.146189

Elapsed: 0.146274
-------------

This shows that the test run matches the baseline.

We don't want the test to live in `current` so we'll move it over to the appropriate test set. In
this case `quick`.
------------
mv test-files/cmd/current/Example* test-files/cmd/quick/
------------

*_Inconsistent Output_*

Sometimes scripts have output values that change from run to run such as data/time stamps. Many of
these values get stripped out automatically, but if there is something relevant to just your test
you can remove it via grep/sed. If that isn't an option you may need to modify ScriptTest.cpp to
be knowledgeable of your situation. Be careful, because it will modify the way that all tests are
verified.

*_Additional Details_*

* All command line tests should add `Testing.conf` as their final configuration file and then add
any configuration option customization necessary for the test after it. 
* Most tests should run at the `--warn` log level by default.
* Tests making use of the Hooteanny API services database should start with the prefix `Service`.
* Tests testing code in `hoot-rnd` should start with the prefix `Rnd`.
* Tests within the `hoot-rnd` module are generally always run in the `--glacial` test suite.
* Tests which use the single test OSM API database instance or have any other reason to run in 
isolation should be run serially by placing them in the "serial" folder.
* If you write a class that uses caching, make the cache size configurable and set the size to the 
lowest possible value in order for tests to pass within `Testing.conf`.

===== Case Tests (Micro Conflate)

Frequently it is desirable to test one aspect of the conflation routines. E.g. did the names get
merged properly? Did two buildings get matched/merged? etc. The micro conflate tests are designed
to help with this. These are not, "Did it conflate all of DC exactly the same?" tests or "Did
these 15 roads get conflated properly?" tests. They're intended to test one situation for
correctness. Primarily they're tiny so they don't all break constantly, and it is very easy to
determine what happened.

These tests are discovered/created from `test-files/cases`. The test creation process goes as 
follows:

* Search `test-files/cases` for a config file (`Config.conf`), if there is one, push it
onto the config file stack.
* If there are directories, recursively search them for tests, but ignore any directories that end
with `.off`
* If there are no directories, search for `Input1.osm`, `Input2.osm`, and `Expected.osm`. If they're
found, then create a new test case for this directory.

When a test runs it runs as follows:

* Load all the config files in turn starting with the highest level directory config file.
* Verify that the test has all the required files.
* Run the equivalent of a conflate command on the two input files and put the result in `Output.osm`.
* Verify that `Expected.osm` matches `Output.osm`.

This approach makes it very fast/easy to create new micro tests and run them with the rest of the
test routines. At this time the micro tests run as part of _quick_ and up.

To only run case tests execute:
------------
HootTest --case-only
------------

See `test-files/cases/README` for additional information on case tests.

===== Translation Tests

Formerly known as the "Plugins" tests, these test a variety of `hoot-js` functionality. Not all of 
the tests are strictly translation related. Tests may be invoked in isolation with:
--------------
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure
make -sj$(nproc) translations-test
--------------

To run an individual test:
--------------
cd $HOOT_HOME/translations/test
mocha <test name>.js
--------------

===== Regression Tests

The Regression Tests run Hootenanny command line operations against specific datasets to measure
Hootenanny performance against particular scenarios. The tests are run against non-public data and,
therefore, are kept in a private Maxar repository and run on a nightly basis only. For
more information about the tests, create an issue at https://github.com/ngageoint/hootenanny.

Many of the regression tests score Hootenanny's conflation accuracy on a dataset and mark the test
as passing or failing based on an allowable score range.  See
hoot-tests/release_test.child/jakarta-spaghetti.release as an example.

===== Test Output Validation

Hootenanny has the capability to run validation routines on test output and generate a report with 
validation failures. A benefit of running the test output validation is that new validation errors 
introduced by conflation may be caught before the software is released to production. See the 
section "Using Validation Checks to Improve Conflated Maps" in the Developer Documentation for more
information on validating test output. It also contains detail on configuring 
https://josm.openstreetmap.de/[JOSM] for use with Hootenanny.

To enable validation during testing, enable the `test.validation.enable` configuration option. 
Any test may have its output validated but in practice we limit the number of tests whose output is
validated to avoid adding unnecessary test runtime. Currently, all case tests and selected script 
tests have their output validated when `test.validation.enable=true`.

The memory used by JNI can be controlled from the entries in `conf/core/Testing.conf` for the 
configuration options `jni.initial.memory` and `jni.max.memory`. The maximum memory is set to the 
lowest possible setting that does not cause out of memory errors when the tests are run. The value 
may have to periodically be adjusted.

NOTE: Currently, there is a bug where `test.validation.enable` must be set in 
`ConfigOptions.asciidoc` and it is not read from `Testing.conf` or from the command line by 
`HootTest` (may only occur when tests are run in parallel).

NOTE: Many of the initial baseline validation reports contain validation errors. For the initial
implementation of test output validation it has been assumed that the starting output files were a 
good baseline, despite any errors. Going forward, the errors may either be accepted or prevented via
conflation code improvements.

*_Case Tests_*

Case tests make calls to `TestOutputValidator` to validate their output. Details:

* A case test must contain either a file named `validation-report` containing the baseline (gold) 
validation report for the test output or a file named `validation-report.off` indicating that test 
output validation should be skipped for the test. 
* If test output validation is enabled and the actual test validation report (`validation-report` 
file written to the test output directory) does not match the baseline validation report, the test 
will fail. 
* Case tests output a copy of the test output file, `Output-validated.osm` derived from 
`Output.osm`, with validation error tags attached to any involved features. This file may be useful 
for debugging. 
* Any other CPPUnit test can have its output validated using `TestOutputValidator`, if desired, 
however some modifications to the code may need to be made to `TestOutputValidator` to do so.

*_Script Tests_*

Script tests may voluntarily elect to have test output validated. To avoid excessive core test suite 
runtimes, only script tests running conflation and generating smaller outputs (<100k?) should have 
validation enabled. If validation adds too much runtime to a script test, it may have to be moved 
to a different suite (e.g. moved from the "slow" suite to the "glacial" suite). Generally, though, 
that practice should be avoided when possible. 

The excerpt below taken from `ConflateCmdTest.sh` shows how to use validation within a script test:
--------------
source ./$HOOT_HOME/scripts/core/ScriptUtils.sh
validateTestOutput $OUT_DIR/output.osm $OUT_DIR/output-validation-report \
  $OUT_DIR/output-validated.osm $IN_DIR_2/output-validation-report
--------------

Details:

Looking at the function `validateTestOutput` in `ScriptTestUtils.sh`:
* The empty file, `test-output/test-validation-enabled`, is checked for existence before running 
validation from a script test. `HootTest` will automatically create this dummy file if the 
pre-requisites for running validation are met (`test.validation.enable=true`). If the pre-requisites 
have not been met, this file will be removed by `HootTest` and scripts will skip running validation. 
This is the needed due to the fact the script has no way itself of knowing if the validation 
pre-requisites have been met.
* Read the `validateTestOutput` function header for information on the function input variables.
* A diff is performed against the test output validation report and a baseline validation report. 
Any differences will show up in the test output and cause the test to fail.
* There is no set naming convention for validation report and validated output files. Just ensure 
that if running multiple validations within a single script tests that all file names are unique.

*_Running Only Tests Which Use Validation_*

The `--validated-only` only option passed to `HootTest` allows you to only run tests which perform 
test output validation when `test.validation.enable` is activated. This may be useful for debugging
purposes. Example:
--------------
HootTest --warn --validated-only
--------------

===== Running Tests in Parallel

Hootenanny can run Core tests in parallel. Tests in the Translation suite are excluded, however. 
This is accomplished with the `HootTest` option, `--parallel [n]`, where the optional `[n]` 
specifies the number of worker processes to spawn.  Leaving off the `[n]` parameter makes an 
implicit call to the operating system to get the total number of online processing units (The same 
as `$(nproc)` ).
--------------
# Runs quick tests serially
HootTest --quick

# Runs quick tests with two parallel processes
HootTest --quick --parallel 2

# Runs quick tests in parallel with one process per processing unit
HootTest --quick --parallel
--------------

These worker processes are QProcess objects that spawn `HootTest --listen`.  This "listening" 
process accepts single unit test names (similar to `--single`) from standard in, runs the test and 
then sends an end-of-test output command. The master process listens for this command and once 
received it sends another single unit test from the queue. Once the queue is empty, the master 
process shuts down the listen processes and ends.

All tests, other than those in the aforementioned test suites, are capable of running in parallel 
with `--parallel`. See the "Core Unit Tests" and "Core Command Line Tests" sections for details 
about forcing tests in those suites to run in series when necessary. Case tests may not be force to 
run in series when specifying `--parallel` (#4908).

Serial unit tests are all passed off to the first worker process and run serially while the rest of 
the tests are run on the other parallel processes.  Once the first worker process completes the 
serial tests it will continue to help the other processes with the parallel queue.

NOTE: While creating new unit tests every effort should be made to allow for the tests to be run
in parallel. Do not reuse output filenames to avoid stomping on files. Input files can be reused
though.

Regression tests are not run by `HootTest` but may be run in parallel via `make`. e.g. 
`make -j<nproc>`.

==== Web Services Tests

The Web Services tests test the Hootenanny web services interface.  There are two types of
Hootenanny web services tests.  One type is written in Java and use JUnit, Jersey, and a
combination of Mockito, PowerMock, EasyMock for mock objects.  One JUnit test class may contain
multiple JUnit tests.  The other type is written in Javascript and uses a combination of mocha and
chai for testing.

It is important to note that there is also C++ code labeled "services".  That is code which writes
to the Hoot API and OSM API services database and is not affiliated with the Hootenanny Web
Services Java code.

===== Test Suites

Java web services test methods may be placed into either the UnitTest or IntegrationTest categories.
The UnitTest suite corresponds to the slow test suite in the Core Tests, and the IntegrationTest
suite corresponds to the glacial test suite.

To run web services unit tests:
---------
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure --with-services
make -sj$(nproc) test
---------

To run both web services unit and integration tests:
---------
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure --with-services
make -sj$(nproc) test-all
---------

The above commands will run the corresponding Core Tests immediately after the web services test
complete.  There currently is no means to separate the two.

The mocha based web services tests (see node-export-server/test as an example) are not
currently aligned with the test suites.

*_Unit Tests_*

The Web Services Unit Tests are meant to test the Java web service code at the class level.
See hoot-services/src/test/java for test examples.

To mark a web service test method as a Unit Test, place the following annotation in front of the
method declaration:
-----------
@Test
@Category(UnitTest.class)
-----------

*_Integration Tests_*

The Web Services Integration Tests are meant to test the Java web service code across logical
boundaries, such as HTTP, Postgres, OGC, etc.  See hoot-services/src/test/java for test examples.

To mark a web service test method as a Integration Test, place the following annotation in front
of the method declaration:
-----------
@Test
@Category(IntegrationTest.class)
-----------

Unfortunately, we do have quite a few Web Services Tests labeled as Unit Tests which are
technically Integration Tests, since they involve Jersey and Postgres (e.g. MapResourceTest).
The decision was made to leave these are Unit Tests, since they are critical and should be run
with each commit push as part of the slow tests, but those tests should eventually be moved to
the Integration Tests suite and corresponding class level Unit Tests written for them.

==== User Interface Tests

The User Interface tests come in two types. The first type uses Cucumber to test the functionality
of the Hootenanny iD browser based application and its interactions with the Hootenanny Web Services.
The second type uses mocha to test at a more granular level.

===== Cucumber User Interface Tests

The purpose of these tests is to catch relatively simple errors that get introduced into UI 
workflows inadvertently, and not to be a bulletproof set of tests for the user interface. Achieving 
such a thing really isn't feasible.  Also, since these tests exercise code in all three Hootenanny 
codebases, they can quickly reveal inconsistencies between both what the web services expect the 
command line API to be and what it actually is and what the user interface expects the web service 
API to be and what it actually is. With this set of tests in place to catch basic errors, we can 
allow testers to spend more time testing complicated conflation scenarios instead of, for example, 
waiting for a typo on a single line of code to be fixed before they can complete regression testing.

link:$$https://cukes.info$$[Cucumber] is the technology used to simulate browser interactions in 
the tests. Cucumber is the top level interpreter of the
link:$$https://github.com/cucumber/cucumber/wiki/Gherkin$$[gherkin language] that describes each test.
There are many
link:$$https://github.com/cucumber/cucumber/wiki/Tutorials-and-Related-Blog-Posts$$[good tutorials] 
on the web to get you started,

* Hootenanny Cucumber User Interface Tests can be found in test-files/ui.
* Cucumber settings may be changed in `features/support/env.rb`.
* Place common test methods in `features/conflate.feature` and 
`features/step_definitions/custom_steps.rb`.
* Each piece of functionality being tested should be placed into its own *.feature file.
* When running silent mode (`make -s`), Cucumber output will be written to
`test-files/ui/tmp/TestRun.log`.  When running without silent mode, Cucumber test output is written
to the screen.

The User Interface Tests run as part of the glacial test suite by default. You must start Tomcat 
and then deploy the Hootenanny web services and user interface code to Tomcat yourself before 
running these tests, as shown below.

To run the User Interface tests with all other glacial tests:
-----------------------
cd $HOOT_HOME
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure --with-services --with-uitests
make -sj$(nproc)
sudo -u tomcat8 scripts/tomcat/CopyWebAppsToTomcat.sh
make -sj$(nproc) test-all
-----------------------

To run the User Interface Tests by themselves:
-----------------------
cd $HOOT_HOME
# configure step required once per configuration only
aclocal && autoconf && autoheader && automake && ./configure --with-services --with-uitests
make -sj$(nproc)
sudo -u tomcat8 scripts/tomcat/CopyWebAppsToTomcat.sh
make -sj$(nproc) ui-test
-----------------------

You have to add the --with-services option since the UI tests rely on the services to be deployed 
before they're run. The tests will fail with an error message otherwise.

If a test errors out, you'll see the error message on the screen if you're not running in silent 
mode (-s).  If you are running the tests in silent mode, then you can look in test-files/ui/tmp for 
the error log. Browser screenshots should also get written out in the tests directory when tests 
fail, if that's helpful.

When writing tests, try to avoid creating test scenarios that are likely to change over time due to 
changes in other parts of the hoot code.  e.g. A test that expects an exact number of reviews from 
a conflation job.  However, this type of thing is not always completely possible to avoid in order 
to write good tests.

===== Mocha User Interface Tests

These tests reside in hoot-ui/test/spec/hoot and are not known to be currently supported.  They
may be run with:

---------------
cd hoot-ui/test/spec/hoot
npm install
npm test
---------------

==== Load Tests

NOTE: The load tests have been inactive for awhile with no current plans to revive them.

The Load Tests test the scalability of the Hootenanny web services code and are run as part of the
nightly tests in a private Maxar repository. These tests currently are not meant to be run
in a local development environment. When run, the tests output an image with graph metrics on 
Hootenanny scalability for increasing levels of simulated users.  Here is an example set of test 
metrics:

image::images/LoadLatest.png[]

==== Test Coverage Reporting

Reports can be generated which detail the test coverage of Hootenanny code from the command line.
Hootenanny Core code coverage is supplied `gcov` and `lcov`. Java code coverage is supplied by 
http://cobertura.github.io/cobertura/[Cobertura] via Maven.  

Alternatively, you can configure your Jenkins pull request job to generate a test coverage 
report (core only) by selecting: "Build with Parameters" and checking the "Core_coverage" and 
"Configure_coverage" checkboxes. The coverage report can be downloaded from the Jenkins job page as 
`coverage.tar.gz`.

Coverage report generation has the pre-requisite of running all tests associated with the code 
being profiled. For the Java Web Services code, this is done automatically by Cobertura, but for 
the Hootenanny Core code, 'make test' must be made explicit as described below.

To generate a report for just the Hootenanny Core code:
----------------
./configure --with-rnd --with-josm --with-services --with-coverage && make clean-coverage && make -j$(nproc) && HootTest --glacial --parallel && make -j$(nproc) core-coverage
----------------

To generate a report for just the Hootenanny Java web services code:
----------------
./configure --with-services --with-coverage && make services-clean-coverage && make -j$(nproc) services-coverage
----------------

To generate a report for all Hootenanny Core code and the Hootenanny Java Web Services code 
together:
----------------
./configure --with-rnd --with-josm --with-services --with-coverage && make clean-coverage && make -j$(nproc) && make -j$(nproc) test-all && make -j$(nproc) coverage
----------------

The test coverage reports will be output to _$HOOT_HOME/coverage_.

Additional notes:
* Running tests `--with-coverage` may signficantly slow down their runtime and increase their memory
usage. Therefore, its recommended to use `--with-coverage` only when you need to generate a test
coverage report, and you may need to use a lower `--parallel` test count in conjunction with it.
* To get a code coverage report for *all* Hootenanny code, you *must* run 'make test-all' before 
running 'make coverage' for the Hootenanny Core code or you will get incomplete results. The Java 
Web Services 'make services-coverage' command doesn't actually require 'make test' to be run 
beforehand since it is done automatically, but if you run 'make coverage' when generating coverage 
reporting using the `--with-services` configuration option, you should always run 'make test' 
beforehand in order to not receive inaccurate reporting for the Hootenanny Core code.
* For Hootenanny Core code coverage reporting, to get a totally accurate coverage report you need to 
run with all compile configuration options enabled (--with-rnd, --with-services, etc.), and either 
run 'make test-all', or if you don't want to run the Java web services tests then run something 
like: 'HootTest --glacial --parallel && make translations-test'.

==== Test Writing Guidelines

===== General

* Unit tests should strive to test at the single class level only, when possible.
* Unit tests should have nearly a one to one mapping to each class in code. Use the code coverage
report to see where your tests are deficient.
* Unit tests should avoid interfacing with external entities, when possible. e.g. databases,
web servers. Such tests that interface with external entities should then become integration tests 
instead.
* Unit tests should cover as many exceptional error handling cases as is reasonable.
* Use clear test method names to state what you are testing.
* Make gratuitous use of asserts during testing.
* Use comments in test methods where its not obvious in the code how/why you're testing something.
* Small amounts of test data should be used for testing if possible. Do not check very large test 
data files into the repository. Keep test files to <= 1MB when possible.
* Do not overwrite generated test output used to verify a test unless you are sure that in doing
so you are still preserving the integrity of the test.
* In Java, mock objects are your friends when writing tests.
* Design a class so that testing of all of its members is possible. In some cases, you may need
to expose members only to the tests. e.g. Use C++ friend keyword, etc.; or in Java, Mockito may
help with this.
* Do not access a Singleton's private members from a test. This is tempting in writing tests but may 
lead to instabilities when the tests are run in parallel.

===== Core Specific

* Use the variable name `uut` to denote the class being tested in a test.
* During testing you can verify test output via CPPUnit asserts against the state of the output data 
or via file comparison (`HOOT_FILE_EQUALS`) of the output with known good output. An advantage to 
using file comparison for testing output is that the test code is less verbose and tedious to 
maintain as the class evolves. A disadvantage of using file comparison is that it is not always 
clear what the intentions of your test are and individuals can inadvertently overwrite your intended 
test output if they do not understand why they broke the test. Weigh these pros and cons when 
selecting which one of these test output verification methods you will use.
* When configuring tests that run conflation, run them with the minimal number of conflate matchers
that are necessary. e.g. If only conflating roads, only use a road matcher. This helps increase the 
overall runtime performance of the tests and reduce their memory footprint.
* Care should be taken to categorize tests based on the amount of time they complete. e.g. don't
put a longer running test in the C++ quick tests.
* Keep test using address data with libpostal confined to a single CPPUnit test (see Testing with 
Address Data section).

===== Services Specific

* For the Java tests, longer running tests should most likely be put into the integration tests.

