
=== Introduction

Note that the some of the hyperlinks in this document only are navigable when the document is in PDF form.

Mapping data is a commodity with a demand that continues to increase with the
growth of geographically-enabled applications.  Mapping data is generated by
commercial, government, and more recently by crowd-sourced methods such as
<<OpenStreetMap,OpenStreetMap>> and WikiMapia footnote:[ WikiMapia or Wikimapia
is an open-content collaborative mapping project that aims to mark and
describe all geographical objects in the world. http://wikimapia.org ]. The
challenge is that no one source has it all, but collectively multiple resources
provide the best picture.   "Conflation of maps refers to a combining
of two digital map files to produce a third map file which is 'better' than each
of the component source maps." (<<saalfeld1987,Saalfeld 1985>>) A conflated
dataset is critical to making a better map and is also the foundation to more
effective geospatial analytics (i.e., geocoding, routing).

The task of selecting which dataset(s) to use and merge becomes a daunting process
often requiring a case-by-case evaluation where analysts are forced to resort to
multiple ad-hoc conflation techniques (automated and manual) which quickly becomes
labor intensive and stove-piped. As such, multiple conflation efforts occur in
isolated projects by numerous users for specific purposes with varying workflows,
rules, schemas, formats, and data quality. This type of approach creates inefficiencies
and lacks the standardization that is required for large scale organizational use,
maintenance, and development activities.

To this end, Hootenanny was developed to provide an open source standards based
approach to geospatial vector-data conflation. Hootenanny is designed to facilitate
automated and semi-automated conflation of critical Foundation GEOINT features in
the topographic domain, namely roads (polylines), buildings (polygons), and
points-of-interest (POIs) (points). Conflation happens at the dataset level,
where the user’s workflow determines the best reference dataset and source content,
geometry and attributes, to transfer to the output map. The input data must be
normalized to allow processing and matching of features and attributes from different
schemas. Hootenanny internal processing leverages the key value pair structure of
OpenStreetMap (OSM) for improved utility and applicability to broader user groups,
e.g. normalized attributes can be used to aid in feature matching and OSM’s free
tagging system allows the map to include an unlimited number of attributes describing
each feature (OpenStreetMap/Map Features, 2015).

Historically, conflation applications have been specialized desktop tools which
required a niche expertise with reoccurring licensing requirements and difficult
to customize or add functionality. However, open source software provides an
attractive business model where the user community can contribute, customize,
share, and help maintain the software in an interactive environment. For this
reason, Hootenanny is being developed under the open source General Public License
(GPL) and will be hosted on the National Geospatial-Intelligence Agency’s (NGA)
https://github.com/ngageoint/hootenanny[GitHub website] (NGA GitHub, 2015).

==== History

[quote, Saalfeld, 1987]
____________________________________________________________________
_"Having more than one set of maps to conflate is only now being experienced by
users outside of the Bureau of the Census. Digital files were very expensive to
create and, as a result, were very expensive to buy; and users generally did not
have, nor did they seek, access to more than one digital file of the same
region."_
____________________________________________________________________

This quote comes from a report made when the Census Bureau was preparing for the
1990 Census. At that time, the Census Bureau was researching and developing
methods to merge data from the United Stated Geological Survey into the GBF/DIME
to create what eventually became the TIGER database for the 1990 Census. The
goal of the integration effort was to "consolidate their diverse source
materials into a single coherent map" (<<saalfeld1987,Saalfeld 1987>>).

===== NGA R&D Framework Paper 

In 2009, the National Technology Alliance prepared a paper titled "Fusion in an
R&D environment" (National Technology Alliance, 2009)for the NGA.  This paper
highlights:

* A R&D roadmap for multi-source, multi-modal data fusion, a major component of
  which is conflation of vector datasets. 
* A functional analysis of conflation techniques
* A comprehensive census of existing conflation tools

This paper provided valuable context and direction to this effort.

==== Variants of Conflation

There are three main forms of Conflation:

* Raster-to-raster conflation – rectifies differences in georeferenced rasters
* Raster-to-vector conflation – matches raster and vector data to improve the
  georeferencing of the vector or raster data
* Vector-to-vector conflation – matches vector-to-vector to improve the data in
  one or both vector data sets

NOTE: Hootenanny is currently focused on vector-to-vector conflation.

===== Conflation implementation

Vector conflation can be executed in two ways: fused and related.  From a
conventional standpoint, the conflation of vector datasets is implemented by
correlating one or more overlapping datasets to produce a unified database that
contains the best available geometry and attributes (National Technology
Alliance, 2009), (Linna Li, 2011).  The results of this method are typically
easier to manage and can be used to produce better maps in a straightforward
fashion, in addition to providing a better baseline for analytic functions such
as routing and geocoding.  Edwards and Scott presented a concept that uses
feature matching/correlation while maintaining a feature-level relationship
between datasets, rather than merging the datasets into one (Edwards, 2002).
Such an approach has a less destructive impact on the original data whereby the
geometry and attributes are not merged, but instead the matching features
between datasets are related to one another through a database relationship.

===== Conflation across multiple dimensions

Geometry matching is closely associated with feature matching in the conflation
process.  In this type of processing, features are considered similar based on
proximity, orientation, or even topology.  In most cases, geometric processing
provides a reasonable link between similar geographic features, but there are
cases where geometry alone cannot distinguish between a railroad and a highway
that run parallel to each other.  Given this possibility, Sehgal suggests
implementing multi-dimensional feature matching, specifically combining
geometric matching to feature attributes and names (Sehgal, 2006).

===== Attributes/Semantics/Ontology

Feature attributes and related semantic/ontological relationships can be used to
correlate features between two datasets to extend geometry capabilities. 
According to Adams et al (2010) "More recently the matching of the semantics of
geospatial features has been identified as a key component of the conflation
problem" (Adams Benjamin, 2010)In particular, this technique would help weed out
possible matches that could link very different features to each other (i.e., a
highway to a railroad).

_Formal name variants_

In cases where formal names exist for features, their similarities can be used
to link features to each other. Linna Li, 2011 and Sehgal, 2006 suggest using
Levenshtein distance (VI, 1966) to measure the closeness of feature names, then
merging these similarities into the scores that characterize the geometric
similarities.

==== Potential Contributions to the Conflation R&D Agenda

National Technology Alliance suggests a need for "… a service-oriented
architecture to call conflation functions from other data fusion software via
open interfaces" (National Technology Alliance, 2009). Such services would
enable conflation capabilities to be widely available in conjunction with common
desktop Geographic Information Systems (GIS) applications.  With this potential
comes an increased need for performance and scalability.    To support this
requirement, large conflation jobs can be divided into smaller components and
addressed and processed in parallel (Linna Li, 2011). Such an approach would
lend itself well to modern cloud computing techniques that are designed to
operate on discrete tasks in parallel.

==== Scope/Rationale of Effort

The purpose of this research is developing a fully automated conflation
capability that scales to support big geographic datasets. The rationale is that
such a conflation capability could create a consolidated data set that leverages
feature content from diverse inputs without involving expensive manual conflation.
While the quality of the output will not be the same as manually conflated data,
the cost should be much lower and the quality good enough for many uses.

==== Use Cases

The following use cases highlight where automated conflation can improve
efficiency and reduce cost.

===== Mission specific datasets (MSDS) stored locally on analyst workstations

These data are a result of analysts compiling, storing, and using geospatial
feature data stored locally. These data sets are usually considered "one-off,"
since their usage is typically limited to a single use or user.  While these
data may not be collected using an extraction specification such as FACC
footnote:[Feature and Attribute Coding Catalogue] or NFDD footnote:[Feature
Data Dictionary], they nonetheless could contain additional geometry and/or
attribution that could enhance a global dataset.  Conflation tools could be used
to streamline the integration process and reduce the cost for analysts to upload
their MSDS into a "global view."

===== Spotlight updates

These updates represent the integration of an enhanced dataset that is generally
produced on a larger mapping scale.  Merging Urban Feature Data (UFD) with
Foundation Feature Data (FFD) would be an example of this, where the conflation
process would result in integrating high-resolution "urban scale" interurban
features into moderate scale geospatial data. 

===== Multi-source integration

Sometimes, analysts have many datasets of similar scale to choose from. When
this is the case, the individual datasets may introduce unique features that
collectively provide more complete attribution and geometry of features. A fully
automated tool to accomplish this could provide another option when deciding how
best to weigh the low cost of automated conflation versus the higher quality
that is sometimes yielded by the manual process.

===== Integrate database change sets

Datasets such as OpenStreetMap (OSM) are constantly being edited, so maintaining
internally modified copies is impractical. However, using a fully automated
conflation tool to conflate internal NGA data with OSM could be performed
periodically over global data sets. This approach would begin to enable a
recurrent temporal correction to NGA data.

